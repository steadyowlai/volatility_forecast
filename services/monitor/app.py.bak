"""
Simplified Volatility Forecasting Monitor

Loads everything from master_dataset.parquet and model pickles.
No separate helper functions - just straightforward logic.
"""

import numpy as np
import pandas as pd
from sklearn.metrics import mean_squared_error
from datetime import datetime
from storage import Storage

storage = Storage()


def make_predictions(df, model):
    """Make predictions using model ensemble"""
    features = model['feature_cols']
    X = df[features].values
    
    xgb_pred = model['xgb_model'].predict(X)
    lgbm_pred = model['lgbm_model'].predict(X)
    X_meta = np.column_stack([xgb_pred, lgbm_pred])
    
    return model['meta_model'].predict(X_meta)


def main():
    print("="*70)
    print("Dual Model Monitoring")
    print("="*70)
    
    # Load everything
    print("\nLoading data and models...")
    master_df = storage.read_parquet("data/master_dataset.parquet")
    master_df['date'] = pd.to_datetime(master_df['date'])
    master_df = master_df.sort_values('date')
    print(f"âœ“ Master dataset: {len(master_df)} samples ({master_df['date'].min().date()} to {master_df['date'].max().date()})")
    
    model_a = storage.read_pickle("models/model_90pct.pkl")
    model_b = storage.read_pickle("models/model_100pct.pkl")
    print(f"âœ“ Model A (90%): {len(model_a['feature_cols'])} features")
    print(f"âœ“ Model B (100%): {len(model_b['feature_cols'])} features")
    
    val_baseline = storage.read_json("models/validation_baseline.json")
    dual_baseline = storage.read_json("models/dual_model_baseline.json")
    baseline_rmse = val_baseline['validation_rmse']
    model_a_cutoff = pd.to_datetime(dual_baseline['model_a']['training_date_end'])
    model_b_cutoff = pd.to_datetime(dual_baseline['model_b']['training_date_end'])
    print(f"âœ“ Baselines loaded (Model A trained to {model_a_cutoff.date()}, Model B to {model_b_cutoff.date()})")
    
    # Part 1: Drift Detection
    print("\n" + "="*70)
    print("Part 1: Drift Detection (Model A Validation Set)")
    print("="*70)
    
    val_df = master_df[master_df['date'] > model_a_cutoff]
    
    if len(val_df) == 0:
        print("No validation data available")
        drift_pct = 0.0
    else:
        print(f"Validation: {len(val_df)} samples ({val_df['date'].min().date()} to {val_df['date'].max().date()})")
        
        val_pred = make_predictions(val_df, model_a)
        val_actual = val_df['rv_5d'].values
        current_rmse = np.sqrt(mean_squared_error(val_actual, val_pred))
        drift_pct = ((current_rmse / baseline_rmse) - 1) * 100
        
        print(f"\nCurrent RMSE: {current_rmse:.6f}")
        print(f"Baseline RMSE: {baseline_rmse:.6f}")
        print(f"Drift: {drift_pct:+.2f}%")
        
        if abs(drift_pct) >= 10:
            print("ðŸš¨ ALERT: Drift >= 10%")
        elif abs(drift_pct) >= 5:
            print("âš ï¸  WARNING: Drift >= 5%")
        else:
            print("âœ… OK: Drift < 5%")
    
    # Part 2: Model Comparison
    print("\n" + "="*70)
    print("Part 2: Model Comparison (New Data)")
    print("="*70)
    
    latest_cutoff = max(model_a_cutoff, model_b_cutoff)
    new_df = master_df[master_df['date'] > latest_cutoff]
    
    if len(new_df) == 0:
        print(f"No new data after {latest_cutoff.date()}")
        storage.append_jsonl({
            'timestamp': datetime.now().isoformat(),
            'drift': {'pct': drift_pct, 'status': 'ok' if abs(drift_pct) < 5 else 'warning'},
            'comparison': {'status': 'no_new_data'},
            'recommendation': 'USE MODEL A (90%)'
        }, "data/predictions/monitoring_history.jsonl")
        return
    
    print(f"New data: {len(new_df)} samples ({new_df['date'].min().date()} to {new_df['date'].max().date()})")
    
    # Compare models on different windows
    print(f"\n{'Window':<25} {'Samples':>8} {'Model A':>12} {'Model B':>12} {'Winner':>10}")
    print("-"*70)
    
    comparisons = []
    windows = [
        ('All new data', new_df),
        ('Last 30', new_df.tail(30) if len(new_df) >= 30 else None),
        ('Last 10', new_df.tail(10) if len(new_df) >= 10 else None),
        ('Latest', new_df.tail(1))
    ]
    
    for name, window_df in windows:
        if window_df is None or len(window_df) == 0:
            continue
            
        y_actual = window_df['rv_5d'].values
        y_pred_a = make_predictions(window_df, model_a)
        y_pred_b = make_predictions(window_df, model_b)
        
        rmse_a = np.sqrt(mean_squared_error(y_actual, y_pred_a))
        rmse_b = np.sqrt(mean_squared_error(y_actual, y_pred_b))
        winner = "Model A" if rmse_a < rmse_b else "Model B"
        
        comparisons.append({'winner': winner, 'rmse_a': rmse_a, 'rmse_b': rmse_b})
        print(f"{name:<25} {len(window_df):>8} {rmse_a:>12.6f} {rmse_b:>12.6f} {winner:>10}")
    
    print("-"*70)
    
    # Decision
    model_b_wins = sum(1 for c in comparisons if c['winner'] == "Model B")
    win_rate = (model_b_wins / len(comparisons) * 100) if comparisons else 0
    
    print(f"\nModel B win rate: {model_b_wins}/{len(comparisons)} = {win_rate:.1f}%")
    
    print("\n" + "="*70)
    print("Decision")
    print("="*70)
    print(f"Model A drift: {drift_pct:+.2f}%")
    print(f"Model B win rate: {win_rate:.1f}%")
    
    if win_rate >= 75 and abs(drift_pct) < 10:
        recommendation = "USE MODEL B (100%)"
        reason = f"Model B wins {win_rate:.1f}% >= 75% and drift < 10%"
    else:
        recommendation = "USE MODEL A (90%)"
        reason = f"Win rate {win_rate:.1f}% or drift {abs(drift_pct):.1f}% above threshold"
    
    print(f"\nRecommendation: {recommendation}")
    print(f"Reason: {reason}")
    print("="*70)
    
    # Save record
    storage.append_jsonl({
        'timestamp': datetime.now().isoformat(),
        'drift': {'pct': drift_pct, 'status': 'alert' if abs(drift_pct) >= 10 else ('warning' if abs(drift_pct) >= 5 else 'ok')},
        'comparison': {
            'model_a_rmse': np.mean([c['rmse_a'] for c in comparisons]),
            'model_b_rmse': np.mean([c['rmse_b'] for c in comparisons]),
            'winner': 'Model B' if model_b_wins > len(comparisons)/2 else 'Model A',
            'win_rate': win_rate
        },
        'recommendation': recommendation,
        'reason': reason
    }, "data/predictions/monitoring_history.jsonl")
    
    print("\nâœ… Monitoring complete - record saved")


if __name__ == "__main__":
    main()
