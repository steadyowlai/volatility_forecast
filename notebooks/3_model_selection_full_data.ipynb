{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e35ee0dc",
   "metadata": {},
   "source": [
    "# Model Selection - Full Data Training\n",
    "\n",
    "**Objective:** Train models on 100% of available data instead of holding out 10% for validation.\n",
    "\n",
    "**Motivation:**\n",
    "- In time series forecasting, the most recent data is the most important\n",
    "- Traditional train/val split (90/10) withholds the most relevant data from training\n",
    "- We should train on ALL available historical data and validate on truly future data\n",
    "\n",
    "**Approach:**\n",
    "1. Load master dataset (all 3,962 samples)\n",
    "2. Use time-series cross-validation for model evaluation\n",
    "3. Train final model on 100% of data\n",
    "4. Compare with previous 90/10 split results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380e077f",
   "metadata": {},
   "source": [
    "## 1. Load Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09605d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import Ridge\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783c9294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load master dataset\n",
    "MASTER_DATASET = Path(\"../data/master_dataset.parquet\")\n",
    "\n",
    "df = pd.read_parquet(MASTER_DATASET)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "print(f\"Dataset loaded: {len(df)} samples\")\n",
    "print(f\"Date range: {df['date'].min().date()} to {df['date'].max().date()}\")\n",
    "print(f\"Features: {len(df.columns) - 2}\")  # Exclude date and rv_5d\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d06fc6",
   "metadata": {},
   "source": [
    "## 2. Load Previous Results for Comparison\n",
    "\n",
    "Load the results from the 90/10 split experiment to compare performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43dff9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previous ensemble results (90/10 split)\n",
    "with open('ensemble_final_metrics_2010.json', 'r') as f:\n",
    "    previous_results = json.load(f)\n",
    "\n",
    "print(\"Previous Results (90/10 split):\")\n",
    "print(f\"  Training samples: {previous_results.get('train_samples', 'N/A')}\")\n",
    "print(f\"  Validation samples: {previous_results.get('val_samples', 'N/A')}\")\n",
    "print(f\"  Validation RMSE: {previous_results.get('val_rmse', 'N/A'):.6f}\")\n",
    "print(f\"  Validation MAE: {previous_results.get('val_mae', 'N/A'):.6f}\")\n",
    "print(f\"  Validation R¬≤: {previous_results.get('val_r2', 'N/A'):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463cc889",
   "metadata": {},
   "source": [
    "## 3. Time-Series Cross-Validation Setup\n",
    "\n",
    "Instead of a single train/test split, we'll use **TimeSeriesSplit** to evaluate model performance properly:\n",
    "- Split data into multiple train/test folds moving forward in time\n",
    "- Each fold trains on past data and tests on future data\n",
    "- This simulates realistic forecasting scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23efcf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "feature_cols = [col for col in df.columns if col not in ['date', 'rv_5d']]\n",
    "X = df[feature_cols].values\n",
    "y = df['rv_5d'].values\n",
    "dates = df['date'].values\n",
    "\n",
    "print(f\"Features: {len(feature_cols)}\")\n",
    "print(f\"Samples: {len(X)}\")\n",
    "print(f\"Feature columns: {feature_cols}\")\n",
    "\n",
    "# Setup time-series cross-validation\n",
    "# Use 5 splits to evaluate model stability\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "print(f\"\\nTime Series Cross-Validation: {tscv.n_splits} folds\")\n",
    "for i, (train_idx, test_idx) in enumerate(tscv.split(X)):\n",
    "    train_dates = dates[train_idx]\n",
    "    test_dates = dates[test_idx]\n",
    "    print(f\"\\nFold {i+1}:\")\n",
    "    print(f\"  Train: {len(train_idx)} samples, {pd.to_datetime(train_dates[0]).date()} to {pd.to_datetime(train_dates[-1]).date()}\")\n",
    "    print(f\"  Test:  {len(test_idx)} samples, {pd.to_datetime(test_dates[0]).date()} to {pd.to_datetime(test_dates[-1]).date()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb168a7",
   "metadata": {},
   "source": [
    "## 4. Evaluate Models with Time-Series CV\n",
    "\n",
    "Evaluate each base model using time-series cross-validation to see how they perform across different time periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d01b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best hyperparameters from previous experiments\n",
    "with open('xgb_best_params_2010.json', 'r') as f:\n",
    "    xgb_params = json.load(f)\n",
    "\n",
    "with open('lgbm_best_params_2010.json', 'r') as f:\n",
    "    lgbm_params = json.load(f)\n",
    "\n",
    "with open('linear_params_2010.json', 'r') as f:\n",
    "    ridge_params = json.load(f)\n",
    "\n",
    "print(\"Loaded hyperparameters from previous experiments\")\n",
    "print(f\"XGBoost params: {xgb_params}\")\n",
    "print(f\"LightGBM params: {lgbm_params}\")\n",
    "print(f\"Ridge params: {ridge_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b401ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_cv(model, X, y, tscv, model_name):\n",
    "    \"\"\"Evaluate model using time-series cross-validation.\"\"\"\n",
    "    cv_scores = {'rmse': [], 'mae': [], 'r2': []}\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(tscv.split(X)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        # Train and predict\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        cv_scores['rmse'].append(rmse)\n",
    "        cv_scores['mae'].append(mae)\n",
    "        cv_scores['r2'].append(r2)\n",
    "        \n",
    "        print(f\"  Fold {fold+1}: RMSE={rmse:.6f}, MAE={mae:.6f}, R¬≤={r2:.4f}\")\n",
    "    \n",
    "    # Calculate mean and std\n",
    "    results = {\n",
    "        'rmse_mean': np.mean(cv_scores['rmse']),\n",
    "        'rmse_std': np.std(cv_scores['rmse']),\n",
    "        'mae_mean': np.mean(cv_scores['mae']),\n",
    "        'mae_std': np.std(cv_scores['mae']),\n",
    "        'r2_mean': np.mean(cv_scores['r2']),\n",
    "        'r2_std': np.std(cv_scores['r2'])\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{model_name} CV Results:\")\n",
    "    print(f\"  RMSE: {results['rmse_mean']:.6f} ¬± {results['rmse_std']:.6f}\")\n",
    "    print(f\"  MAE:  {results['mae_mean']:.6f} ¬± {results['mae_std']:.6f}\")\n",
    "    print(f\"  R¬≤:   {results['r2_mean']:.4f} ¬± {results['r2_std']:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Evaluation function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e87df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate XGBoost\n",
    "print(\"=\"*60)\n",
    "print(\"Evaluating XGBoost\")\n",
    "print(\"=\"*60)\n",
    "xgb_model = xgb.XGBRegressor(**xgb_params, random_state=42)\n",
    "xgb_cv_results = evaluate_model_cv(xgb_model, X, y, tscv, \"XGBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45510f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate LightGBM\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Evaluating LightGBM\")\n",
    "print(\"=\"*60)\n",
    "lgbm_model = lgb.LGBMRegressor(**lgbm_params, random_state=42, verbose=-1)\n",
    "lgbm_cv_results = evaluate_model_cv(lgbm_model, X, y, tscv, \"LightGBM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad039ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Ridge\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Evaluating Ridge Regression\")\n",
    "print(\"=\"*60)\n",
    "ridge_model = Ridge(**ridge_params, random_state=42)\n",
    "ridge_cv_results = evaluate_model_cv(ridge_model, X, y, tscv, \"Ridge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6a7da0",
   "metadata": {},
   "source": [
    "## 5. Compare CV Results with Previous 90/10 Split\n",
    "\n",
    "Let's see how cross-validation performance compares to the single 90/10 split approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af9cbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['XGBoost', 'LightGBM', 'Ridge'],\n",
    "    'CV_RMSE': [\n",
    "        xgb_cv_results['rmse_mean'],\n",
    "        lgbm_cv_results['rmse_mean'],\n",
    "        ridge_cv_results['rmse_mean']\n",
    "    ],\n",
    "    'CV_Std': [\n",
    "        xgb_cv_results['rmse_std'],\n",
    "        lgbm_cv_results['rmse_std'],\n",
    "        ridge_cv_results['rmse_std']\n",
    "    ],\n",
    "    'Previous_90/10_RMSE': [\n",
    "        previous_results.get('val_rmse', np.nan),\n",
    "        previous_results.get('val_rmse', np.nan),\n",
    "        previous_results.get('val_rmse', np.nan)\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CROSS-VALIDATION vs 90/10 SPLIT COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(comparison.to_string(index=False))\n",
    "print(\"\\nNote: CV RMSE is averaged across 5 time-series folds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ee0cad",
   "metadata": {},
   "source": [
    "## 6. Dual Model Strategy: Train Model A (90%) and Model B (100%)\n",
    "\n",
    "**Strategy:** Train two models and compare them empirically:\n",
    "- **Model A (90%)**: Conservative model with validation baseline for drift detection\n",
    "- **Model B (100%)**: Aggressive model using all available data for maximum performance\n",
    "\n",
    "We'll compare both models on multiple time windows to see which performs better in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba9928e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data splits\n",
    "train_cutoff = int(len(df) * 0.9)\n",
    "train_df_90 = df[:train_cutoff]\n",
    "val_df = df[train_cutoff:]\n",
    "\n",
    "print(\"Data Splits:\")\n",
    "print(f\"\\nModel A Training (90%): {len(train_df_90)} samples\")\n",
    "print(f\"  Date range: {train_df_90['date'].min().date()} to {train_df_90['date'].max().date()}\")\n",
    "\n",
    "print(f\"\\nValidation Set (10%): {len(val_df)} samples\")\n",
    "print(f\"  Date range: {val_df['date'].min().date()} to {val_df['date'].max().date()}\")\n",
    "\n",
    "print(f\"\\nModel B Training (100%): {len(df)} samples\")\n",
    "print(f\"  Date range: {df['date'].min().date()} to {df['date'].max().date()}\")\n",
    "\n",
    "# Prepare feature matrices\n",
    "X_train_90 = train_df_90[feature_cols].values\n",
    "y_train_90 = train_df_90['rv_5d'].values\n",
    "\n",
    "X_val = val_df[feature_cols].values\n",
    "y_val = val_df['rv_5d'].values\n",
    "\n",
    "X_full = df[feature_cols].values\n",
    "y_full = df['rv_5d'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b05d27",
   "metadata": {},
   "source": [
    "### 6.1 Train Model A: Ensemble on 90% Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df55da1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TRAINING MODEL A (90% DATA)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train base models on 90% data\n",
    "xgb_model_a = xgb.XGBRegressor(**xgb_params, random_state=42)\n",
    "lgbm_model_a = lgb.LGBMRegressor(**lgbm_params, random_state=42, verbose=-1)\n",
    "\n",
    "print(\"\\nTraining XGBoost (Model A)...\")\n",
    "xgb_model_a.fit(X_train_90, y_train_90)\n",
    "print(\"‚úì XGBoost trained\")\n",
    "\n",
    "print(\"Training LightGBM (Model A)...\")\n",
    "lgbm_model_a.fit(X_train_90, y_train_90)\n",
    "print(\"‚úì LightGBM trained\")\n",
    "\n",
    "# Generate base predictions for meta-learner\n",
    "print(\"\\nGenerating meta-features...\")\n",
    "xgb_pred_train_a = xgb_model_a.predict(X_train_90)\n",
    "lgbm_pred_train_a = lgbm_model_a.predict(X_train_90)\n",
    "X_meta_train_a = np.column_stack([xgb_pred_train_a, lgbm_pred_train_a])\n",
    "\n",
    "# Train meta-learner (Ridge)\n",
    "print(\"Training meta-learner (Ridge)...\")\n",
    "meta_model_a = Ridge(**ridge_params, random_state=42)\n",
    "meta_model_a.fit(X_meta_train_a, y_train_90)\n",
    "print(\"‚úì Meta-learner trained\")\n",
    "\n",
    "print(\"\\n‚úì Model A (Ensemble) training complete!\")\n",
    "\n",
    "# Validate on 10% holdout\n",
    "print(\"\\nValidating Model A on holdout set...\")\n",
    "xgb_pred_val_a = xgb_model_a.predict(X_val)\n",
    "lgbm_pred_val_a = lgbm_model_a.predict(X_val)\n",
    "X_meta_val_a = np.column_stack([xgb_pred_val_a, lgbm_pred_val_a])\n",
    "y_pred_val_a = meta_model_a.predict(X_meta_val_a)\n",
    "\n",
    "rmse_val_a = np.sqrt(mean_squared_error(y_val, y_pred_val_a))\n",
    "mae_val_a = mean_absolute_error(y_val, y_pred_val_a)\n",
    "r2_val_a = r2_score(y_val, y_pred_val_a)\n",
    "\n",
    "print(f\"\\nModel A Validation Results:\")\n",
    "print(f\"  RMSE: {rmse_val_a:.6f}\")\n",
    "print(f\"  MAE:  {mae_val_a:.6f}\")\n",
    "print(f\"  R¬≤:   {r2_val_a:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf9d20d",
   "metadata": {},
   "source": [
    "### 6.2 Train Model B: Ensemble on 100% Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2b681f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING MODEL B (100% DATA)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train base models on 100% data\n",
    "xgb_model_b = xgb.XGBRegressor(**xgb_params, random_state=42)\n",
    "lgbm_model_b = lgb.LGBMRegressor(**lgbm_params, random_state=42, verbose=-1)\n",
    "\n",
    "print(\"\\nTraining XGBoost (Model B)...\")\n",
    "xgb_model_b.fit(X_full, y_full)\n",
    "print(\"‚úì XGBoost trained\")\n",
    "\n",
    "print(\"Training LightGBM (Model B)...\")\n",
    "lgbm_model_b.fit(X_full, y_full)\n",
    "print(\"‚úì LightGBM trained\")\n",
    "\n",
    "# Generate base predictions for meta-learner\n",
    "print(\"\\nGenerating meta-features...\")\n",
    "xgb_pred_train_b = xgb_model_b.predict(X_full)\n",
    "lgbm_pred_train_b = lgbm_model_b.predict(X_full)\n",
    "X_meta_train_b = np.column_stack([xgb_pred_train_b, lgbm_pred_train_b])\n",
    "\n",
    "# Train meta-learner (Ridge)\n",
    "print(\"Training meta-learner (Ridge)...\")\n",
    "meta_model_b = Ridge(**ridge_params, random_state=42)\n",
    "meta_model_b.fit(X_meta_train_b, y_full)\n",
    "print(\"‚úì Meta-learner trained\")\n",
    "\n",
    "print(\"\\n‚úì Model B (Ensemble) training complete!\")\n",
    "\n",
    "# Evaluate on validation set (for comparison with Model A)\n",
    "print(\"\\nEvaluating Model B on same validation set...\")\n",
    "xgb_pred_val_b = xgb_model_b.predict(X_val)\n",
    "lgbm_pred_val_b = lgbm_model_b.predict(X_val)\n",
    "X_meta_val_b = np.column_stack([xgb_pred_val_b, lgbm_pred_val_b])\n",
    "y_pred_val_b = meta_model_b.predict(X_meta_val_b)\n",
    "\n",
    "rmse_val_b = np.sqrt(mean_squared_error(y_val, y_pred_val_b))\n",
    "mae_val_b = mean_absolute_error(y_val, y_pred_val_b)\n",
    "r2_val_b = r2_score(y_val, y_pred_val_b)\n",
    "\n",
    "print(f\"\\nModel B Performance on Validation Set:\")\n",
    "print(f\"  RMSE: {rmse_val_b:.6f}\")\n",
    "print(f\"  MAE:  {mae_val_b:.6f}\")\n",
    "print(f\"  R¬≤:   {r2_val_b:.4f}\")\n",
    "\n",
    "print(f\"\\nNote: Model B was trained on this data, so these metrics are\")\n",
    "print(f\"      in-sample (not a true validation). Use for comparison only.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ce9517",
   "metadata": {},
   "source": [
    "### 6.3 Compare Both Models on Multiple Time Windows\n",
    "\n",
    "Now let's compare how both models perform on different time windows to see which one is actually better in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd94336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ensemble(xgb_model, lgbm_model, meta_model, X):\n",
    "    \"\"\"Helper function to make ensemble predictions.\"\"\"\n",
    "    xgb_pred = xgb_model.predict(X)\n",
    "    lgbm_pred = lgbm_model.predict(X)\n",
    "    X_meta = np.column_stack([xgb_pred, lgbm_pred])\n",
    "    return meta_model.predict(X_meta)\n",
    "\n",
    "def compare_models_on_window(window_name, window_df):\n",
    "    \"\"\"Compare both models on a specific time window.\"\"\"\n",
    "    X_window = window_df[feature_cols].values\n",
    "    y_true = window_df['rv_5d'].values\n",
    "    \n",
    "    # Model A predictions\n",
    "    y_pred_a = predict_ensemble(xgb_model_a, lgbm_model_a, meta_model_a, X_window)\n",
    "    rmse_a = np.sqrt(mean_squared_error(y_true, y_pred_a))\n",
    "    mae_a = mean_absolute_error(y_true, y_pred_a)\n",
    "    \n",
    "    # Model B predictions\n",
    "    y_pred_b = predict_ensemble(xgb_model_b, lgbm_model_b, meta_model_b, X_window)\n",
    "    rmse_b = np.sqrt(mean_squared_error(y_true, y_pred_b))\n",
    "    mae_b = mean_absolute_error(y_true, y_pred_b)\n",
    "    \n",
    "    # Comparison\n",
    "    winner = 'B' if rmse_b < rmse_a else 'A'\n",
    "    improvement = (rmse_a - rmse_b) / rmse_a * 100 if winner == 'B' else (rmse_b - rmse_a) / rmse_b * 100\n",
    "    \n",
    "    return {\n",
    "        'window': window_name,\n",
    "        'samples': len(window_df),\n",
    "        'date_start': window_df['date'].min().date(),\n",
    "        'date_end': window_df['date'].max().date(),\n",
    "        'model_a_rmse': rmse_a,\n",
    "        'model_a_mae': mae_a,\n",
    "        'model_b_rmse': rmse_b,\n",
    "        'model_b_mae': mae_b,\n",
    "        'winner': winner,\n",
    "        'improvement_pct': abs(improvement)\n",
    "    }\n",
    "\n",
    "print(\"Comparison functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e2c557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define time windows for comparison\n",
    "windows = [\n",
    "    (\"Full Validation Set (10%)\", val_df),\n",
    "    (\"Last 30 samples\", df.tail(30)),\n",
    "    (\"Last 10 samples\", df.tail(10)),\n",
    "    (\"Last 1 sample\", df.tail(1))\n",
    "]\n",
    "\n",
    "# Compare models on each window\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DUAL MODEL COMPARISON - MULTIPLE TIME WINDOWS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = []\n",
    "for window_name, window_df in windows:\n",
    "    result = compare_models_on_window(window_name, window_df)\n",
    "    results.append(result)\n",
    "    \n",
    "    print(f\"\\n{window_name}:\")\n",
    "    print(f\"  Samples: {result['samples']}\")\n",
    "    print(f\"  Date Range: {result['date_start']} to {result['date_end']}\")\n",
    "    print(f\"  Model A RMSE: {result['model_a_rmse']:.6f}\")\n",
    "    print(f\"  Model B RMSE: {result['model_b_rmse']:.6f}\")\n",
    "    print(f\"  Winner: Model {result['winner']} (better by {result['improvement_pct']:.2f}%)\")\n",
    "\n",
    "# Create summary dataframe\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY TABLE\")\n",
    "print(\"=\"*80)\n",
    "print(results_df[['window', 'samples', 'model_a_rmse', 'model_b_rmse', 'winner', 'improvement_pct']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581d0846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: RMSE Comparison\n",
    "window_names = [r['window'] for r in results]\n",
    "model_a_rmses = [r['model_a_rmse'] for r in results]\n",
    "model_b_rmses = [r['model_b_rmse'] for r in results]\n",
    "\n",
    "x = np.arange(len(window_names))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, model_a_rmses, width, label='Model A (90%)', alpha=0.8)\n",
    "axes[0].bar(x + width/2, model_b_rmses, width, label='Model B (100%)', alpha=0.8)\n",
    "axes[0].set_xlabel('Time Window')\n",
    "axes[0].set_ylabel('RMSE')\n",
    "axes[0].set_title('RMSE Comparison Across Time Windows')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels([w.replace(' ', '\\n') for w in window_names], fontsize=8)\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 2: Win Count\n",
    "win_counts = results_df['winner'].value_counts()\n",
    "colors = ['#2ecc71' if w == 'B' else '#3498db' for w in win_counts.index]\n",
    "axes[1].bar(win_counts.index, win_counts.values, color=colors, alpha=0.8)\n",
    "axes[1].set_xlabel('Model')\n",
    "axes[1].set_ylabel('Number of Wins')\n",
    "axes[1].set_title('Model Performance: Win Count')\n",
    "axes[1].set_xticks([0, 1])\n",
    "axes[1].set_xticklabels(['Model A\\n(90%)', 'Model B\\n(100%)'])\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, v in enumerate(win_counts.values):\n",
    "    axes[1].text(i, v + 0.05, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nModel B wins: {len([r for r in results if r['winner'] == 'B'])}/4 windows\")\n",
    "print(f\"Average improvement when Model B wins: {np.mean([r['improvement_pct'] for r in results if r['winner'] == 'B']):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffda741",
   "metadata": {},
   "source": [
    "## 7. Decision Logic and Recommendations\n",
    "\n",
    "Based on the dual model comparison, let's establish decision rules for production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f490ddea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate drift for Model A (using validation baseline)\n",
    "drift_pct = (results[1]['model_a_rmse'] / rmse_val_a - 1) * 100  # Using last 30 samples\n",
    "\n",
    "# Decision logic\n",
    "model_b_win_rate = len([r for r in results if r['winner'] == 'B']) / len(results) * 100\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PRODUCTION DECISION LOGIC\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nModel A (90%) Baseline: {rmse_val_a:.6f} RMSE\")\n",
    "print(f\"Model A Drift (last 30): {drift_pct:+.1f}%\")\n",
    "print(f\"Model B Win Rate: {model_b_win_rate:.0f}% ({len([r for r in results if r['winner'] == 'B'])}/4 windows)\")\n",
    "\n",
    "print(\"\\nDecision Rules:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Rule 1: Check drift\n",
    "if drift_pct > 20:\n",
    "    print(\"üî¥ CRITICAL DRIFT DETECTED (>20%)\")\n",
    "    print(\"   ‚Üí RETRAIN BOTH MODELS IMMEDIATELY\")\n",
    "    recommendation = \"RETRAIN\"\n",
    "elif drift_pct > 10:\n",
    "    print(\"‚ö†Ô∏è  WARNING: Moderate drift detected (10-20%)\")\n",
    "    print(\"   ‚Üí Plan retraining within next week\")\n",
    "    recommendation = \"PLAN_RETRAIN\"\n",
    "else:\n",
    "    print(\"‚úì Drift is acceptable (<10%)\")\n",
    "    recommendation = \"OK\"\n",
    "\n",
    "# Rule 2: Model selection\n",
    "print(f\"\\nModel Selection:\")\n",
    "if model_b_win_rate >= 75 and drift_pct < 10:\n",
    "    print(\"‚úì Use Model B (100% data) for production\")\n",
    "    print(f\"  Reason: Wins {model_b_win_rate:.0f}% of windows and drift is low\")\n",
    "    selected_model = \"B\"\n",
    "elif model_b_win_rate >= 50:\n",
    "    print(\"‚ö†Ô∏è  Mixed results - Consider using Model B but monitor closely\")\n",
    "    selected_model = \"B (with caution)\"\n",
    "else:\n",
    "    print(\"‚úì Use Model A (90% data) for production\")\n",
    "    print(\"  Reason: More consistent performance, have validation baseline\")\n",
    "    selected_model = \"A\"\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"FINAL RECOMMENDATION: Use Model {selected_model}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2d5da9",
   "metadata": {},
   "source": [
    "## 8. Save Both Models and Baselines\n",
    "\n",
    "Save both models and their performance metrics for production use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e6d9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "# Save Model A (90%)\n",
    "model_a_artifact = {\n",
    "    'xgb_model': xgb_model_a,\n",
    "    'lgbm_model': lgbm_model_a,\n",
    "    'meta_model': meta_model_a,\n",
    "    'feature_cols': feature_cols\n",
    "}\n",
    "\n",
    "with open('model_90pct.pkl', 'wb') as f:\n",
    "    pickle.dump(model_a_artifact, f)\n",
    "print(\"‚úì Model A saved: model_90pct.pkl\")\n",
    "\n",
    "# Save Model B (100%)\n",
    "model_b_artifact = {\n",
    "    'xgb_model': xgb_model_b,\n",
    "    'lgbm_model': lgbm_model_b,\n",
    "    'meta_model': meta_model_b,\n",
    "    'feature_cols': feature_cols\n",
    "}\n",
    "\n",
    "with open('model_100pct.pkl', 'wb') as f:\n",
    "    pickle.dump(model_b_artifact, f)\n",
    "print(\"‚úì Model B saved: model_100pct.pkl\")\n",
    "\n",
    "# Save dual baseline metrics\n",
    "dual_baseline = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'model_a': {\n",
    "        'training_samples': len(train_df_90),\n",
    "        'training_date_start': str(train_df_90['date'].min().date()),\n",
    "        'training_date_end': str(train_df_90['date'].max().date()),\n",
    "        'validation_rmse': float(rmse_val_a),\n",
    "        'validation_mae': float(mae_val_a),\n",
    "        'validation_r2': float(r2_val_a),\n",
    "        'validation_samples': len(val_df)\n",
    "    },\n",
    "    'model_b': {\n",
    "        'training_samples': len(df),\n",
    "        'training_date_start': str(df['date'].min().date()),\n",
    "        'training_date_end': str(df['date'].max().date()),\n",
    "        'validation_rmse': float(rmse_val_b),\n",
    "        'validation_mae': float(mae_val_b),\n",
    "        'validation_r2': float(r2_val_b),\n",
    "        'note': 'Model B metrics on validation set are in-sample'\n",
    "    },\n",
    "    'comparison': {\n",
    "        'model_b_win_rate': float(model_b_win_rate),\n",
    "        'recommended_model': selected_model\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('dual_model_baseline.json', 'w') as f:\n",
    "    json.dump(dual_baseline, f, indent=2)\n",
    "print(\"‚úì Dual baseline saved: dual_model_baseline.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL ARTIFACTS SAVED\")\n",
    "print(\"=\"*80)\n",
    "print(\"Files created:\")\n",
    "print(\"  - model_90pct.pkl\")\n",
    "print(\"  - model_100pct.pkl\")\n",
    "print(\"  - dual_model_baseline.json\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
