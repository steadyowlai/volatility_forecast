{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cfbde89",
   "metadata": {},
   "source": [
    "# Data prep: deterministic train/val/test splits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4923805e",
   "metadata": {},
   "source": [
    "Load `data/master_dataset.parquet`, sort by date, do light exploration, and make deterministic train/val/test splits for other notebooks.\n",
    "\n",
    "Artifacts live under `notebooks/model_evaluation_final/artifacts/`:\n",
    "- `data/` for splits, indices, config, feature list\n",
    "\n",
    "Each model notebook should handle its own scaling or preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd382e3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/aayushrijal/Documents/GitHub/volatility_forecast/data/master_dataset.parquet')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#paths\n",
    "_start = Path.cwd().resolve()\n",
    "_candidates = [_start] + list(_start.parents)\n",
    "_repo = None\n",
    "for p in _candidates:\n",
    "    if (p / \"data/master_dataset.parquet\").exists():\n",
    "        _repo = p\n",
    "        break\n",
    "REPO_ROOT = _repo if _repo else _start\n",
    "\n",
    "MASTER_PATH = REPO_ROOT / \"data/master_dataset.parquet\"\n",
    "ARTIFACTS_DIR = REPO_ROOT / \"notebooks/model_evaluation_final/artifacts\"\n",
    "DATA_DIR = ARTIFACTS_DIR / \"data\"\n",
    "for d in [ARTIFACTS_DIR, DATA_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CONFIG = {\n",
    "    \"test_days\": 30,  #hold out the most recent n rows as test\n",
    "    \"val_fraction_pre_test\": 0.2,  #fraction of pre-test data for validation\n",
    "    \"target_col\": \"rv_5d\",\n",
    "    \"date_col\": \"date\",\n",
    "    \"drop_cols\": [\"symbol\"],  #dropped if present\n",
    "    \"scaler\": \"standard\"\n",
    "}\n",
    "\n",
    "with open(DATA_DIR / \"split_config.json\", \"w\") as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "\n",
    "MASTER_PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81b05af7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(        date  spy_ret_1d  spy_ret_5d  spy_ret_10d  spy_ret_20d  spy_ret_60d  \\\n",
       " 0 2010-03-31   -0.003413    0.001369     0.003255     0.045110     0.035979   \n",
       " 1 2010-04-01    0.006814    0.009810     0.010582     0.048901     0.040150   \n",
       " 2 2010-04-05    0.008116    0.018527     0.023773     0.042825     0.047563   \n",
       " 3 2010-04-06    0.002355    0.014554     0.020796     0.045005     0.045705   \n",
       " 4 2010-04-07   -0.005729    0.008144     0.008059     0.037615     0.036654   \n",
       " \n",
       "    spy_vol_5d  spy_vol_10d  spy_vol_20d  spy_vol_60d  ...      vix3m  \\\n",
       " 0    0.007427     0.013506     0.023040     0.068533  ...  19.920000   \n",
       " 1    0.009947     0.015119     0.023836     0.068820  ...  19.900000   \n",
       " 2    0.012824     0.016392     0.020799     0.069294  ...  19.350000   \n",
       " 3    0.011400     0.015679     0.020931     0.069206  ...  18.840000   \n",
       " 4    0.012740     0.015150     0.021637     0.069363  ...  19.190001   \n",
       " \n",
       "    vix_term  rsi_spy_14  corr_spy_tlt_20d  corr_spy_hyg_20d  corr_spy_tlt_60d  \\\n",
       " 0  1.132462   67.495451         -0.033182          0.662313         -0.405897   \n",
       " 1  1.139096   71.402005         -0.050730          0.572254         -0.413883   \n",
       " 2  1.136898   74.947225          0.037414          0.370970         -0.436396   \n",
       " 3  1.160813   72.625995          0.023980          0.323784         -0.437734   \n",
       " 4  1.154633   62.684577         -0.138809          0.284130         -0.445640   \n",
       " \n",
       "    corr_spy_hyg_60d  hyg_tlt_spread  rv_vix_spread_20d     rv_5d  \n",
       " 0          0.748294       -0.010002         -17.566960  0.012753  \n",
       " 1          0.739700        0.000029         -17.446164  0.012612  \n",
       " 2          0.735433        0.016320         -16.999202  0.009783  \n",
       " 3          0.728954        0.004897         -16.209069  0.009524  \n",
       " 4          0.721787       -0.011228         -16.598364  0.013611  \n",
       " \n",
       " [5 rows x 22 columns],\n",
       "            date  spy_ret_1d  spy_ret_5d  spy_ret_10d  spy_ret_20d  \\\n",
       " 3967 2026-01-07   -0.003229    0.003734     0.006912     0.011616   \n",
       " 3968 2026-01-08   -0.000102    0.011069     0.002250     0.012378   \n",
       " 3969 2026-01-08   -0.000102    0.011069     0.002250     0.012378   \n",
       " 3970 2026-01-09    0.006592    0.015829     0.005331     0.012360   \n",
       " 3971 2026-01-09    0.006592    0.015829     0.005331     0.012360   \n",
       " \n",
       "       spy_ret_60d  spy_vol_5d  spy_vol_10d  spy_vol_20d  spy_vol_60d  ...  \\\n",
       " 3967     0.057426    0.012178     0.013988     0.025907     0.057025  ...   \n",
       " 3968     0.042096    0.009644     0.013225     0.025892     0.054954  ...   \n",
       " 3969     0.042096    0.009644     0.013225     0.025892     0.054954  ...   \n",
       " 3970     0.049910    0.011537     0.014353     0.025888     0.055335  ...   \n",
       " 3971     0.049910    0.011537     0.014353     0.025888     0.055335  ...   \n",
       " \n",
       "           vix3m  vix_term  rsi_spy_14  corr_spy_tlt_20d  corr_spy_hyg_20d  \\\n",
       " 3967  18.590000  1.208713   62.973157          0.365087          0.683113   \n",
       " 3968  18.389999  1.190291   74.139623          0.364227          0.692110   \n",
       " 3969  18.389999  1.190291   74.139623          0.364227          0.692110   \n",
       " 3970  17.879999  1.233954   73.828268          0.382328          0.675920   \n",
       " 3971  17.879999  1.233954   73.828268          0.382328          0.675920   \n",
       " \n",
       "       corr_spy_tlt_60d  corr_spy_hyg_60d  hyg_tlt_spread  rv_vix_spread_20d  \\\n",
       " 3967          0.150407          0.715084       -0.006074         -15.354094   \n",
       " 3968          0.157620          0.690577        0.005766         -15.424107   \n",
       " 3969          0.157618          0.690579        0.005766         -15.424107   \n",
       " 3970          0.177839          0.692938       -0.005877         -14.464112   \n",
       " 3971          0.177837          0.692941       -0.005877         -14.464112   \n",
       " \n",
       "          rv_5d  \n",
       " 3967  0.008614  \n",
       " 3968  0.009033  \n",
       " 3969  0.009033  \n",
       " 3970  0.006233  \n",
       " 3971  0.006233  \n",
       " \n",
       " [5 rows x 22 columns],\n",
       " (3972, 22))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert MASTER_PATH.exists(), f\"Missing master dataset at {MASTER_PATH}\"\n",
    "df = pd.read_parquet(MASTER_PATH)\n",
    "\n",
    "#normalize column types\n",
    "df[CONFIG[\"date_col\"]] = pd.to_datetime(df[CONFIG[\"date_col\"]])\n",
    "df = df.sort_values(CONFIG[\"date_col\"]).reset_index(drop=True)\n",
    "\n",
    "df.head(), df.tail(), df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "743f3b7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_rows': 3154,\n",
       " 'val_rows': 788,\n",
       " 'test_rows': 30,\n",
       " 'train_start': datetime.date(2010, 3, 31),\n",
       " 'train_end': datetime.date(2022, 10, 7),\n",
       " 'val_start': datetime.date(2022, 10, 10),\n",
       " 'val_end': datetime.date(2025, 11, 28),\n",
       " 'test_start': datetime.date(2025, 12, 1),\n",
       " 'test_end': datetime.date(2026, 1, 9),\n",
       " 'n_features': 20}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_col = CONFIG[\"date_col\"]\n",
    "target_col = CONFIG[\"target_col\"]\n",
    "drop_cols = [c for c in CONFIG[\"drop_cols\"] if c in df.columns]\n",
    "feature_cols = [c for c in df.columns if c not in [date_col, target_col] + drop_cols]\n",
    "\n",
    "#determines split sizes\n",
    "test_days = CONFIG[\"test_days\"]\n",
    "pre_test = df.iloc[:-test_days] if test_days > 0 else df\n",
    "test = df.iloc[-test_days:] if test_days > 0 else df.iloc[0:0]\n",
    "\n",
    "val_size = max(1, int(len(pre_test) * CONFIG[\"val_fraction_pre_test\"]))\n",
    "val = pre_test.iloc[-val_size:]\n",
    "train = pre_test.iloc[:-val_size]\n",
    "\n",
    "assert len(train) > 0, \"Train set empty; adjust split parameters.\"\n",
    "assert len(val) > 0, \"Val set empty; adjust split parameters.\"\n",
    "assert len(test) >= 0, \"Test set negative size.\"\n",
    "\n",
    "summary = {\n",
    "    \"train_rows\": len(train),\n",
    "    \"val_rows\": len(val),\n",
    "    \"test_rows\": len(test),\n",
    "    \"train_start\": train[date_col].min().date(),\n",
    "    \"train_end\": train[date_col].max().date(),\n",
    "    \"val_start\": val[date_col].min().date(),\n",
    "    \"val_end\": val[date_col].max().date(),\n",
    "    \"test_start\": test[date_col].min().date() if len(test) else None,\n",
    "    \"test_end\": test[date_col].max().date() if len(test) else None,\n",
    "    \"n_features\": len(feature_cols)\n",
    "}\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9334b72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3154, 788, 30, 20)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = train[feature_cols].copy()\n",
    "y_train = train[[target_col]].copy()\n",
    "X_val = val[feature_cols].copy()\n",
    "y_val = val[[target_col]].copy()\n",
    "X_test = test[feature_cols].copy()\n",
    "y_test = test[[target_col]].copy() if len(test) else pd.DataFrame(columns=[target_col])\n",
    "\n",
    "#persist indices(by date)\n",
    "train[[date_col]].to_csv(DATA_DIR / \"train_indices.csv\", index=False)\n",
    "val[[date_col]].to_csv(DATA_DIR / \"val_indices.csv\", index=False)\n",
    "test[[date_col]].to_csv(DATA_DIR / \"test_indices.csv\", index=False)\n",
    "\n",
    "#persist unscaled features/targets\n",
    "X_train.to_parquet(DATA_DIR / \"X_train.parquet\", index=False)\n",
    "X_val.to_parquet(DATA_DIR / \"X_val.parquet\", index=False)\n",
    "X_test.to_parquet(DATA_DIR / \"X_test.parquet\", index=False)\n",
    "y_train.to_parquet(DATA_DIR / \"y_train.parquet\", index=False)\n",
    "y_val.to_parquet(DATA_DIR / \"y_val.parquet\", index=False)\n",
    "y_test.to_parquet(DATA_DIR / \"y_test.parquet\", index=False)\n",
    "\n",
    "with open(DATA_DIR / \"feature_columns.json\", \"w\") as f:\n",
    "    json.dump(feature_cols, f, indent=2)\n",
    "\n",
    "len(X_train), len(X_val), len(X_test), len(feature_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4dc836d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(             rv_5d\n",
       " count  3972.000000\n",
       " mean      0.019514\n",
       " std       0.014571\n",
       " min       0.001851\n",
       " 25%       0.010529\n",
       " 50%       0.016283\n",
       " 75%       0.024191\n",
       " max       0.189043,\n",
       "               count      mean       std       min       25%       50%  \\\n",
       " spy_ret_1d   3972.0  0.000520  0.010881 -0.115886 -0.003711  0.000697   \n",
       " spy_ret_5d   3972.0  0.002599  0.022634 -0.198077 -0.007006  0.004582   \n",
       " spy_ret_10d  3972.0  0.005181  0.030702 -0.265117 -0.007926  0.008302   \n",
       " spy_ret_20d  3972.0  0.010396  0.042515 -0.370872 -0.007888  0.016752   \n",
       " spy_ret_60d  3972.0  0.031478  0.064405 -0.359347  0.004195  0.040915   \n",
       " \n",
       "                   75%       max  \n",
       " spy_ret_1d   0.005776  0.099863  \n",
       " spy_ret_5d   0.014785  0.160060  \n",
       " spy_ret_10d  0.022695  0.172254  \n",
       " spy_ret_20d  0.034642  0.207604  \n",
       " spy_ret_60d  0.071479  0.334965  )"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#quick stats for sanity\n",
    "stats_target = df[[target_col]].describe()\n",
    "stats_features = df[feature_cols].describe().T.head()\n",
    "\n",
    "stats_target, stats_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757ee771",
   "metadata": {},
   "source": [
    "Next: other notebooks load these artifacts. Use the scaler only for linear models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29768684",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
