{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cfbde89",
   "metadata": {},
   "source": [
    "# Data prep: deterministic train/val/test splits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4923805e",
   "metadata": {},
   "source": [
    "Load `data/master_dataset.parquet`, sort by date, do light exploration, and make deterministic train/val/test splits for other notebooks.\n",
    "\n",
    "Artifacts live under `notebooks/model_evaluation_final/artifacts/`:\n",
    "- `data/` for splits, indices, config, feature list\n",
    "\n",
    "Each model notebook should handle its own scaling or preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd382e3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/aayushrijal/Documents/GitHub/volatility_forecast/data/master_dataset.parquet')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#paths\n",
    "_start = Path.cwd().resolve()\n",
    "_candidates = [_start] + list(_start.parents)\n",
    "_repo = None\n",
    "for p in _candidates:\n",
    "    if (p / \"data/master_dataset.parquet\").exists():\n",
    "        _repo = p\n",
    "        break\n",
    "REPO_ROOT = _repo if _repo else _start\n",
    "\n",
    "MASTER_PATH = REPO_ROOT / \"data/master_dataset.parquet\"\n",
    "ARTIFACTS_DIR = REPO_ROOT / \"notebooks/model_evaluation_final/artifacts\"\n",
    "DATA_DIR = ARTIFACTS_DIR / \"data\"\n",
    "for d in [ARTIFACTS_DIR, DATA_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CONFIG = {\n",
    "    \"test_days\": 30,  #hold out the most recent n rows as test\n",
    "    \"val_fraction_pre_test\": 0.2,  #fraction of pre-test data for validation\n",
    "    \"target_col\": \"rv_5d\",\n",
    "    \"date_col\": \"date\",\n",
    "    \"drop_cols\": [\"symbol\"],  #dropped if present\n",
    "    \"scaler\": \"standard\"\n",
    "}\n",
    "\n",
    "with open(DATA_DIR / \"split_config.json\", \"w\") as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "\n",
    "MASTER_PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81b05af7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(        date  spy_ret_1d  spy_ret_5d  spy_ret_10d  spy_ret_20d  spy_ret_60d  \\\n",
       " 0 2010-01-04         NaN         NaN          NaN          NaN          NaN   \n",
       " 1 2010-01-05    0.002643         NaN          NaN          NaN          NaN   \n",
       " 2 2010-01-06    0.000704         NaN          NaN          NaN          NaN   \n",
       " 3 2010-01-07    0.004212         NaN          NaN          NaN          NaN   \n",
       " 4 2010-01-08    0.003322         NaN          NaN          NaN          NaN   \n",
       " \n",
       "    spy_vol_5d  spy_vol_10d  spy_vol_20d  spy_vol_60d  ...      vix3m  \\\n",
       " 0         NaN          NaN          NaN          NaN  ...  22.770000   \n",
       " 1         NaN          NaN          NaN          NaN  ...  22.389999   \n",
       " 2         NaN          NaN          NaN          NaN  ...  21.799999   \n",
       " 3         NaN          NaN          NaN          NaN  ...  21.600000   \n",
       " 4         NaN          NaN          NaN          NaN  ...  21.000000   \n",
       " \n",
       "    vix_term  rsi_spy_14  corr_spy_tlt_20d  corr_spy_hyg_20d  corr_spy_tlt_60d  \\\n",
       " 0  1.136228         NaN               NaN               NaN               NaN   \n",
       " 1  1.157106         NaN               NaN               NaN               NaN   \n",
       " 2  1.137787         NaN               NaN               NaN               NaN   \n",
       " 3  1.133263         NaN               NaN               NaN               NaN   \n",
       " 4  1.158301         NaN               NaN               NaN               NaN   \n",
       " \n",
       "    corr_spy_hyg_60d  hyg_tlt_spread  rv_vix_spread_20d     rv_5d  \n",
       " 0               NaN             NaN                NaN  0.006182  \n",
       " 1               NaN       -0.001704                NaN  0.010910  \n",
       " 2               NaN        0.016058                NaN  0.013758  \n",
       " 3               NaN        0.002348                NaN  0.013373  \n",
       " 4               NaN        0.002010                NaN  0.017182  \n",
       " \n",
       " [5 rows x 22 columns],\n",
       "            date  spy_ret_1d  spy_ret_5d  spy_ret_10d  spy_ret_20d  \\\n",
       " 4043 2026-01-30   -0.002987    0.003968    -0.000390     0.014630   \n",
       " 4044 2026-02-02    0.004959    0.003861     0.005407     0.017758   \n",
       " 4045 2026-02-03   -0.008491   -0.008606     0.017483     0.002629   \n",
       " 4046 2026-02-04   -0.004856   -0.013361     0.001152    -0.008157   \n",
       " 4047 2026-02-05   -0.012568   -0.023943    -0.016626    -0.017496   \n",
       " \n",
       "       spy_ret_60d  spy_vol_5d  spy_vol_10d  spy_vol_20d  spy_vol_60d  ...  \\\n",
       " 4043     0.015501    0.007372     0.025239     0.028488     0.055874  ...   \n",
       " 4044     0.032384    0.007299     0.025708     0.028859     0.054812  ...   \n",
       " 4045     0.020433    0.010468     0.017606     0.029341     0.055357  ...   \n",
       " 4046     0.026365    0.011539     0.014209     0.029143     0.054513  ...   \n",
       " 4047     0.012813    0.016945     0.018240     0.031572     0.055934  ...   \n",
       " \n",
       "           vix3m  vix_term  rsi_spy_14  corr_spy_tlt_20d  corr_spy_hyg_20d  \\\n",
       " 4043  20.070000  1.150803   47.605973          0.586495          0.536118   \n",
       " 4044  19.370001  1.185435   50.270504          0.581592          0.749517   \n",
       " 4045  20.350000  1.130556   45.818546          0.497919          0.756350   \n",
       " 4046  20.620001  1.106223   45.881901          0.533438          0.772613   \n",
       " 4047  22.450001  1.031236   37.247027          0.316999          0.782207   \n",
       " \n",
       "       corr_spy_tlt_60d  corr_spy_hyg_60d  hyg_tlt_spread  rv_vix_spread_20d  \\\n",
       " 4043          0.313983          0.670540        0.006718         -17.411512   \n",
       " 4044          0.331275          0.684298        0.003468         -16.311141   \n",
       " 4045          0.341820          0.683971       -0.003043         -17.970659   \n",
       " 4046          0.414588          0.703737        0.001299         -18.610857   \n",
       " 4047          0.321069          0.705292       -0.011921         -21.738428   \n",
       " \n",
       "       rv_5d  \n",
       " 4043    NaN  \n",
       " 4044    NaN  \n",
       " 4045    NaN  \n",
       " 4046    NaN  \n",
       " 4047    NaN  \n",
       " \n",
       " [5 rows x 22 columns],\n",
       " (4048, 22))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert MASTER_PATH.exists(), f\"Missing master dataset at {MASTER_PATH}\"\n",
    "df = pd.read_parquet(MASTER_PATH)\n",
    "\n",
    "#normalize column types\n",
    "df[CONFIG[\"date_col\"]] = pd.to_datetime(df[CONFIG[\"date_col\"]])\n",
    "df = df.sort_values(CONFIG[\"date_col\"]).reset_index(drop=True)\n",
    "\n",
    "df.head(), df.tail(), df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aec2feb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 1: Drop rows with null rv_5d\n",
      "============================================================\n",
      "Dropped 5 rows with null rv_5d: 4043 rows remaining\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4043, 22)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop rows with null rv_5d (labels not available yet)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 1: Drop rows with null rv_5d\")\n",
    "print(\"=\"*60)\n",
    "original_rows = len(df)\n",
    "df_clean = df.dropna(subset=[CONFIG[\"target_col\"]]).copy()\n",
    "rv_dropped = original_rows - len(df_clean)\n",
    "print(f\"Dropped {rv_dropped} rows with null rv_5d: {len(df_clean)} rows remaining\")\n",
    "\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "743f3b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 2: Create train/val/test splits\n",
      "============================================================\n",
      "Split sizes (before dropping null features):\n",
      "  Train: 3211 rows\n",
      "  Val:   802 rows\n",
      "  Test:  30 rows\n",
      "  Total: 4043 rows\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_rows_before_null_drop': 3211,\n",
       " 'val_rows_before_null_drop': 802,\n",
       " 'test_rows': 30,\n",
       " 'train_start': datetime.date(2010, 1, 4),\n",
       " 'train_end': datetime.date(2022, 10, 4),\n",
       " 'val_start': datetime.date(2022, 10, 5),\n",
       " 'val_end': datetime.date(2025, 12, 15),\n",
       " 'test_start': datetime.date(2025, 12, 16),\n",
       " 'test_end': datetime.date(2026, 1, 29),\n",
       " 'n_features': 20}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 2: Create train/val/test splits\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "date_col = CONFIG[\"date_col\"]\n",
    "target_col = CONFIG[\"target_col\"]\n",
    "drop_cols = [c for c in CONFIG[\"drop_cols\"] if c in df_clean.columns]\n",
    "feature_cols = [c for c in df_clean.columns if c not in [date_col, target_col] + drop_cols]\n",
    "\n",
    "#split after dropping null rv_5d\n",
    "test_days = CONFIG[\"test_days\"]\n",
    "pre_test = df_clean.iloc[:-test_days] if test_days > 0 else df_clean\n",
    "test = df_clean.iloc[-test_days:] if test_days > 0 else df_clean.iloc[0:0]\n",
    "\n",
    "val_size = max(1, int(len(pre_test) * CONFIG[\"val_fraction_pre_test\"]))\n",
    "val = pre_test.iloc[-val_size:]\n",
    "train = pre_test.iloc[:-val_size]\n",
    "\n",
    "assert len(train) > 0, \"Train set empty; adjust split parameters.\"\n",
    "assert len(val) > 0, \"Val set empty; adjust split parameters.\"\n",
    "assert len(test) >= 0, \"Test set negative size.\"\n",
    "\n",
    "print(f\"Split sizes (before dropping null features):\")\n",
    "print(f\"  Train: {len(train)} rows\")\n",
    "print(f\"  Val:   {len(val)} rows\")\n",
    "print(f\"  Test:  {len(test)} rows\")\n",
    "print(f\"  Total: {len(train) + len(val) + len(test)} rows\")\n",
    "\n",
    "summary = {\n",
    "    \"train_rows_before_null_drop\": len(train),\n",
    "    \"val_rows_before_null_drop\": len(val),\n",
    "    \"test_rows\": len(test),\n",
    "    \"train_start\": train[date_col].min().date(),\n",
    "    \"train_end\": train[date_col].max().date(),\n",
    "    \"val_start\": val[date_col].min().date(),\n",
    "    \"val_end\": val[date_col].max().date(),\n",
    "    \"test_start\": test[date_col].min().date() if len(test) else None,\n",
    "    \"test_end\": test[date_col].max().date() if len(test) else None,\n",
    "    \"n_features\": len(feature_cols)\n",
    "}\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1c55f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 3: Drop rows with null features from train/val\n",
      "============================================================\n",
      "Train: dropped 60 rows with null features: 3151 rows remaining\n",
      "Val:   dropped 0 rows with null features: 802 rows remaining\n",
      "Test:  keeping all 30 rows (realistic nulls for evaluation)\n",
      "\n",
      "Final split sizes:\n",
      "  Train: 3151 rows (clean)\n",
      "  Val:   802 rows (clean)\n",
      "  Test:  30 rows (may have nulls)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3151, 802, 30)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop rows with null features from train and val (keep test realistic)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 3: Drop rows with null features from train/val\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "train_before_drop = len(train)\n",
    "train = train.dropna(subset=feature_cols).copy()\n",
    "train_feature_dropped = train_before_drop - len(train)\n",
    "print(f\"Train: dropped {train_feature_dropped} rows with null features: {len(train)} rows remaining\")\n",
    "\n",
    "val_before_drop = len(val)\n",
    "val = val.dropna(subset=feature_cols).copy()\n",
    "val_feature_dropped = val_before_drop - len(val)\n",
    "print(f\"Val:   dropped {val_feature_dropped} rows with null features: {len(val)} rows remaining\")\n",
    "\n",
    "print(f\"Test:  keeping all {len(test)} rows (realistic nulls for evaluation)\")\n",
    "\n",
    "print(f\"\\nFinal split sizes:\")\n",
    "print(f\"  Train: {len(train)} rows (clean)\")\n",
    "print(f\"  Val:   {len(val)} rows (clean)\")\n",
    "print(f\"  Test:  {len(test)} rows (may have nulls)\")\n",
    "\n",
    "len(train), len(val), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9334b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 4: Compute feature means and save artifacts\n",
      "============================================================\n",
      "Computed means for 20 features from training data\n",
      "\n",
      "Saved artifacts to /Users/aayushrijal/Documents/GitHub/volatility_forecast/notebooks/model_evaluation_final/artifacts/data:\n",
      "  - Split indices (train/val/test_indices.csv)\n",
      "  - Features and targets (X/y parquet files)\n",
      "  - feature_means.json (20 features)\n",
      "  - feature_columns.json (20 features)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3151, 802, 30, 20)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 4: Compute feature means and save artifacts\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "X_train = train[feature_cols].copy()\n",
    "y_train = train[[target_col]].copy()\n",
    "X_val = val[feature_cols].copy()\n",
    "y_val = val[[target_col]].copy()\n",
    "X_test = test[feature_cols].copy()\n",
    "y_test = test[[target_col]].copy() if len(test) else pd.DataFrame(columns=[target_col])\n",
    "\n",
    "#compute feature means from training data only (for prediction imputation)\n",
    "feature_means = X_train.mean().to_dict()\n",
    "print(f\"Computed means for {len(feature_means)} features from training data\")\n",
    "\n",
    "#persist indices(by date)\n",
    "train[[date_col]].to_csv(DATA_DIR / \"train_indices.csv\", index=False)\n",
    "val[[date_col]].to_csv(DATA_DIR / \"val_indices.csv\", index=False)\n",
    "test[[date_col]].to_csv(DATA_DIR / \"test_indices.csv\", index=False)\n",
    "\n",
    "#persist unscaled features/targets\n",
    "X_train.to_parquet(DATA_DIR / \"X_train.parquet\", index=False)\n",
    "X_val.to_parquet(DATA_DIR / \"X_val.parquet\", index=False)\n",
    "X_test.to_parquet(DATA_DIR / \"X_test.parquet\", index=False)\n",
    "y_train.to_parquet(DATA_DIR / \"y_train.parquet\", index=False)\n",
    "y_val.to_parquet(DATA_DIR / \"y_val.parquet\", index=False)\n",
    "y_test.to_parquet(DATA_DIR / \"y_test.parquet\", index=False)\n",
    "\n",
    "#persist feature means for prediction imputation\n",
    "with open(DATA_DIR / \"feature_means.json\", \"w\") as f:\n",
    "    json.dump(feature_means, f, indent=2)\n",
    "\n",
    "with open(DATA_DIR / \"feature_columns.json\", \"w\") as f:\n",
    "    json.dump(feature_cols, f, indent=2)\n",
    "\n",
    "print(f\"\\nSaved artifacts to {DATA_DIR}:\")\n",
    "print(f\"  - Split indices (train/val/test_indices.csv)\")\n",
    "print(f\"  - Features and targets (X/y parquet files)\")\n",
    "print(f\"  - feature_means.json ({len(feature_means)} features)\")\n",
    "print(f\"  - feature_columns.json ({len(feature_cols)} features)\")\n",
    "\n",
    "len(X_train), len(X_val), len(X_test), len(feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4dc836d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL SUMMARY\n",
      "============================================================\n",
      "Original rows:        4048\n",
      "Dropped null rv_5d:   5\n",
      "After rv_5d drop:     4043\n",
      "\n",
      "Train rows (final):   3151 (dropped 60 with null features)\n",
      "Val rows (final):     802 (dropped 0 with null features)\n",
      "Test rows:            30 (kept realistic nulls)\n",
      "\n",
      "Features:             20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'original_rows': 4048,\n",
       " 'dropped_null_rv_5d': 5,\n",
       " 'after_rv_5d_drop': 4043,\n",
       " 'train_rows_final': 3151,\n",
       " 'train_feature_dropped': 60,\n",
       " 'val_rows_final': 802,\n",
       " 'val_feature_dropped': 0,\n",
       " 'test_rows': 30,\n",
       " 'train_start': datetime.date(2010, 3, 31),\n",
       " 'train_end': datetime.date(2022, 10, 4),\n",
       " 'val_start': datetime.date(2022, 10, 5),\n",
       " 'val_end': datetime.date(2025, 12, 15),\n",
       " 'test_start': datetime.date(2025, 12, 16),\n",
       " 'test_end': datetime.date(2026, 1, 29),\n",
       " 'n_features': 20}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#updated summary with final counts\n",
    "summary_final = {\n",
    "    \"original_rows\": original_rows,\n",
    "    \"dropped_null_rv_5d\": rv_dropped,\n",
    "    \"after_rv_5d_drop\": len(df_clean),\n",
    "    \"train_rows_final\": len(train),\n",
    "    \"train_feature_dropped\": train_feature_dropped,\n",
    "    \"val_rows_final\": len(val),\n",
    "    \"val_feature_dropped\": val_feature_dropped,\n",
    "    \"test_rows\": len(test),\n",
    "    \"train_start\": train[date_col].min().date(),\n",
    "    \"train_end\": train[date_col].max().date(),\n",
    "    \"val_start\": val[date_col].min().date(),\n",
    "    \"val_end\": val[date_col].max().date(),\n",
    "    \"test_start\": test[date_col].min().date() if len(test) else None,\n",
    "    \"test_end\": test[date_col].max().date() if len(test) else None,\n",
    "    \"n_features\": len(feature_cols)\n",
    "}\n",
    "\n",
    "with open(DATA_DIR / \"split_summary.json\", \"w\") as f:\n",
    "    json.dump(summary_final, f, indent=2, default=str)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Original rows:        {summary_final['original_rows']}\")\n",
    "print(f\"Dropped null rv_5d:   {summary_final['dropped_null_rv_5d']}\")\n",
    "print(f\"After rv_5d drop:     {summary_final['after_rv_5d_drop']}\")\n",
    "print(f\"\\nTrain rows (final):   {summary_final['train_rows_final']} (dropped {summary_final['train_feature_dropped']} with null features)\")\n",
    "print(f\"Val rows (final):     {summary_final['val_rows_final']} (dropped {summary_final['val_feature_dropped']} with null features)\")\n",
    "print(f\"Test rows:            {summary_final['test_rows']} (kept realistic nulls)\")\n",
    "print(f\"\\nFeatures:             {summary_final['n_features']}\")\n",
    "\n",
    "summary_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b65a46b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target variable stats:\n",
      "             rv_5d\n",
      "count  4043.000000\n",
      "mean      0.019483\n",
      "std       0.014487\n",
      "min       0.001851\n",
      "25%       0.010543\n",
      "50%       0.016279\n",
      "75%       0.024170\n",
      "max       0.189043\n",
      "\n",
      "Sample feature stats (first 10):\n",
      "               count      mean       std       min       25%       50%  \\\n",
      "spy_ret_1d    4042.0  0.000519  0.010847 -0.115886 -0.003709  0.000706   \n",
      "spy_ret_5d    4038.0  0.002593  0.022585 -0.198077 -0.006985  0.004582   \n",
      "spy_ret_10d   4033.0  0.005176  0.030680 -0.265117 -0.007947  0.008335   \n",
      "spy_ret_20d   4023.0  0.010458  0.042544 -0.370872 -0.007856  0.016789   \n",
      "spy_ret_60d   3983.0  0.031455  0.064320 -0.359347  0.004383  0.040782   \n",
      "spy_vol_5d    4038.0  0.019494  0.014492  0.001851  0.010550  0.016283   \n",
      "spy_vol_10d   4033.0  0.028456  0.019267  0.003834  0.017027  0.023628   \n",
      "spy_vol_20d   4023.0  0.041302  0.025641  0.009232  0.026084  0.034892   \n",
      "spy_vol_60d   3983.0  0.074675  0.039231  0.024900  0.052995  0.062331   \n",
      "drawdown_60d  3984.0  0.027855  0.040243  0.000000  0.001600  0.011113   \n",
      "\n",
      "                   75%       max  \n",
      "spy_ret_1d    0.005772  0.099863  \n",
      "spy_ret_5d    0.014768  0.160060  \n",
      "spy_ret_10d   0.022768  0.172254  \n",
      "spy_ret_20d   0.034813  0.207604  \n",
      "spy_ret_60d   0.071473  0.334965  \n",
      "spy_vol_5d    0.024186  0.189043  \n",
      "spy_vol_10d   0.034534  0.222119  \n",
      "spy_vol_20d   0.048841  0.260082  \n",
      "spy_vol_60d   0.084233  0.299364  \n",
      "drawdown_60d  0.037014  0.337173  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(             rv_5d\n",
       " count  4043.000000\n",
       " mean      0.019483\n",
       " std       0.014487\n",
       " min       0.001851\n",
       " 25%       0.010543\n",
       " 50%       0.016279\n",
       " 75%       0.024170\n",
       " max       0.189043,\n",
       "                count      mean       std       min       25%       50%  \\\n",
       " spy_ret_1d    4042.0  0.000519  0.010847 -0.115886 -0.003709  0.000706   \n",
       " spy_ret_5d    4038.0  0.002593  0.022585 -0.198077 -0.006985  0.004582   \n",
       " spy_ret_10d   4033.0  0.005176  0.030680 -0.265117 -0.007947  0.008335   \n",
       " spy_ret_20d   4023.0  0.010458  0.042544 -0.370872 -0.007856  0.016789   \n",
       " spy_ret_60d   3983.0  0.031455  0.064320 -0.359347  0.004383  0.040782   \n",
       " spy_vol_5d    4038.0  0.019494  0.014492  0.001851  0.010550  0.016283   \n",
       " spy_vol_10d   4033.0  0.028456  0.019267  0.003834  0.017027  0.023628   \n",
       " spy_vol_20d   4023.0  0.041302  0.025641  0.009232  0.026084  0.034892   \n",
       " spy_vol_60d   3983.0  0.074675  0.039231  0.024900  0.052995  0.062331   \n",
       " drawdown_60d  3984.0  0.027855  0.040243  0.000000  0.001600  0.011113   \n",
       " \n",
       "                    75%       max  \n",
       " spy_ret_1d    0.005772  0.099863  \n",
       " spy_ret_5d    0.014768  0.160060  \n",
       " spy_ret_10d   0.022768  0.172254  \n",
       " spy_ret_20d   0.034813  0.207604  \n",
       " spy_ret_60d   0.071473  0.334965  \n",
       " spy_vol_5d    0.024186  0.189043  \n",
       " spy_vol_10d   0.034534  0.222119  \n",
       " spy_vol_20d   0.048841  0.260082  \n",
       " spy_vol_60d   0.084233  0.299364  \n",
       " drawdown_60d  0.037014  0.337173  )"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#quick stats for sanity\n",
    "stats_target = df_clean[[target_col]].describe()\n",
    "stats_features = df_clean[feature_cols].describe().T.head(10)\n",
    "\n",
    "print(\"\\nTarget variable stats:\")\n",
    "print(stats_target)\n",
    "print(\"\\nSample feature stats (first 10):\")\n",
    "print(stats_features)\n",
    "\n",
    "stats_target, stats_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757ee771",
   "metadata": {},
   "source": [
    "Next: other notebooks load these artifacts. Use the scaler only for linear models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29768684",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
