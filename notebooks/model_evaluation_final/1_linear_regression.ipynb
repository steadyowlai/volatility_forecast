{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a11318b0",
   "metadata": {},
   "source": [
    "# Linear regression (ridge/lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a473376",
   "metadata": {},
   "source": [
    "Load splits from artifacts/data, scale features, run a simple baseline, then a hyperparameter sweep, compare on val, and save outputs under artifacts/linear_regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c8c483d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('/Users/aayushrijal/Documents/GitHub/volatility_forecast/notebooks/model_evaluation_final/artifacts/data'),\n",
       " PosixPath('/Users/aayushrijal/Documents/GitHub/volatility_forecast/notebooks/model_evaluation_final/artifacts/linear_regression'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import TimeSeriesSplit, ParameterGrid\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# paths to data and artifacts\n",
    "_start = Path.cwd().resolve()\n",
    "_candidates = [_start] + list(_start.parents)\n",
    "_repo = None\n",
    "for p in _candidates:\n",
    "    if (p / 'notebooks/model_evaluation_final/artifacts/data/split_config.json').exists():\n",
    "        _repo = p\n",
    "        break\n",
    "    if (p / 'data/master_dataset.parquet').exists():\n",
    "        _repo = p\n",
    "        break\n",
    "REPO_ROOT = _repo if _repo else _start\n",
    "ARTIFACTS_DIR = REPO_ROOT / 'notebooks/model_evaluation_final/artifacts'\n",
    "DATA_DIR = ARTIFACTS_DIR / 'data'\n",
    "MODEL_DIR = ARTIFACTS_DIR / 'linear_regression'\n",
    "for d in [ARTIFACTS_DIR, DATA_DIR, MODEL_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_DIR, MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c43320d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3154, 788, 30, 20)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load artifacts\n",
    "with open(DATA_DIR / 'feature_columns.json') as f:\n",
    "    feature_cols = json.load(f)\n",
    "with open(DATA_DIR / 'split_config.json') as f:\n",
    "    CONFIG = json.load(f)\n",
    "target_col = CONFIG['target_col']\n",
    "date_col = CONFIG['date_col']\n",
    "\n",
    "X_train = pd.read_parquet(DATA_DIR / 'X_train.parquet')\n",
    "X_val = pd.read_parquet(DATA_DIR / 'X_val.parquet')\n",
    "X_test = pd.read_parquet(DATA_DIR / 'X_test.parquet')\n",
    "y_train = pd.read_parquet(DATA_DIR / 'y_train.parquet')[target_col].values\n",
    "y_val = pd.read_parquet(DATA_DIR / 'y_val.parquet')[target_col].values\n",
    "y_test = pd.read_parquet(DATA_DIR / 'y_test.parquet')[target_col].values if (DATA_DIR / 'y_test.parquet').exists() else np.array([])\n",
    "\n",
    "len(X_train), len(X_val), len(X_test), len(feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4027c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3154, 788, 30)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#keep raw features; scaling handled inside pipelines\n",
    "X_train_raw = X_train.copy()\n",
    "X_val_raw = X_val.copy()\n",
    "X_test_raw = X_test.copy()\n",
    "\n",
    "len(X_train_raw), len(X_val_raw), len(X_test_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "279ad3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# small helper functions\n",
    "\n",
    "def metrics(y_true, y_pred):\n",
    "    return {\n",
    "        'rmse': float(np.sqrt(mean_squared_error(y_true, y_pred))),\n",
    "        'mae': float(mean_absolute_error(y_true, y_pred)),\n",
    "        'r2': float(r2_score(y_true, y_pred))\n",
    "    }\n",
    "\n",
    "def eval_model(model, name, X_tr, y_tr, X_v, y_v, X_te=None, y_te=None):\n",
    "    model.fit(X_tr, y_tr)\n",
    "    pred_tr = model.predict(X_tr)\n",
    "    pred_v = model.predict(X_v)\n",
    "    out = {\n",
    "        'name': name,\n",
    "        'train': metrics(y_tr, pred_tr),\n",
    "        'val': metrics(y_v, pred_v)\n",
    "    }\n",
    "    preds = {'train': pred_tr, 'val': pred_v, 'test': None}\n",
    "    if X_te is not None and len(X_te):\n",
    "        pred_te = model.predict(X_te)\n",
    "        out['test'] = metrics(y_te, pred_te)\n",
    "        preds['test'] = pred_te\n",
    "    return out, model, preds\n",
    "\n",
    "def ts_cv_score(model, X, y, n_splits=5):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    scores = []\n",
    "    for train_idx, val_idx in tscv.split(X):\n",
    "        X_tr, X_va = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_tr, y_va = y[train_idx], y[val_idx]\n",
    "        m = clone(model)\n",
    "        m.fit(X_tr, y_tr)\n",
    "        preds = m.predict(X_va)\n",
    "        scores.append(np.sqrt(mean_squared_error(y_va, preds)))\n",
    "    return float(np.mean(scores)), float(np.std(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de9330d",
   "metadata": {},
   "source": [
    "## Baseline (default ridge/lasso)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5beeb095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'ridge_base',\n",
       " 'train': {'rmse': 0.009478733598480976,\n",
       "  'mae': 0.006036318876028565,\n",
       "  'r2': 0.6072369626380871},\n",
       " 'val': {'rmse': 0.00937056442784859,\n",
       "  'mae': 0.005723706201454683,\n",
       "  'r2': 0.41738086626526394},\n",
       " 'test': {'rmse': 0.004635058164696176,\n",
       "  'mae': 0.003977044265744952,\n",
       "  'r2': -0.5855142983167421}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baseline models with default settings\n",
    "baseline_results = []\n",
    "baseline_models = []\n",
    "baseline_candidates = {\n",
    "    'ridge_base': Pipeline([('scaler', StandardScaler()), ('model', Ridge())]),\n",
    "    'lasso_base': Pipeline([('scaler', StandardScaler()), ('model', Lasso(max_iter=5000))])\n",
    "}\n",
    "for name, model in baseline_candidates.items():\n",
    "    out, mdl, preds = eval_model(model, name, X_train_raw, y_train, X_val_raw, y_val, X_test_raw, y_test)\n",
    "    baseline_results.append(out)\n",
    "    baseline_models.append((out, mdl, preds))\n",
    "\n",
    "baseline_best = sorted(baseline_results, key=lambda x: x['val']['rmse'])[0]\n",
    "baseline_best_name = baseline_best['name']\n",
    "baseline_best_tuple = [m for m in baseline_models if m[0]['name'] == baseline_best_name][0]\n",
    "baseline_best_model = baseline_best_tuple[1]\n",
    "baseline_best_preds = baseline_best_tuple[2]\n",
    "baseline_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6f0a62",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning (TimeSeriesSplit + pipelines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e12c868c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ridge': {'cv': {'family': 'ridge',\n",
       "   'params': {'model__alpha': 100.0},\n",
       "   'cv_rmse_mean': 0.00939326355004182,\n",
       "   'cv_rmse_std': 0.003798918829684565},\n",
       "  'metrics': {'name': 'ridge_tuned',\n",
       "   'train': {'rmse': 0.009558150323875733,\n",
       "    'mae': 0.006031938316587894,\n",
       "    'r2': 0.6006279312809666},\n",
       "   'val': {'rmse': 0.00943853028065473,\n",
       "    'mae': 0.005782997477662603,\n",
       "    'r2': 0.40889859992702926},\n",
       "   'test': {'rmse': 0.004489562349565113,\n",
       "    'mae': 0.0037324392363447807,\n",
       "    'r2': -0.48753708126735384}},\n",
       "  'model': Pipeline(steps=[('scaler', StandardScaler()), ('model', Ridge(alpha=100.0))]),\n",
       "  'preds': {'train': array([0.01695871, 0.0164928 , 0.01606074, ..., 0.03332624, 0.03511177,\n",
       "          0.0380785 ]),\n",
       "   'val': array([0.04042721, 0.04218448, 0.04352601, 0.03860229, 0.03863921,\n",
       "          0.03721972, 0.03593324, 0.03642944, 0.03627617, 0.03035978,\n",
       "          0.0293626 , 0.02731585, 0.02643531, 0.02781402, 0.02291757,\n",
       "          0.02581306, 0.0277659 , 0.03040584, 0.03061804, 0.02824773,\n",
       "          0.0274641 , 0.0291201 , 0.03003178, 0.0260826 , 0.02369726,\n",
       "          0.02690666, 0.02741012, 0.02523768, 0.02264079, 0.02264144,\n",
       "          0.02261954, 0.02175402, 0.0180134 , 0.01728571, 0.02302628,\n",
       "          0.0212355 , 0.02038854, 0.01860493, 0.01659933, 0.0209788 ,\n",
       "          0.02510493, 0.02642568, 0.02503218, 0.02604386, 0.02709636,\n",
       "          0.02209782, 0.0217974 , 0.02766097, 0.02832926, 0.02807689,\n",
       "          0.02539964, 0.02289043, 0.02593796, 0.02249486, 0.02441318,\n",
       "          0.02788428, 0.02652441, 0.02361203, 0.02586382, 0.02297893,\n",
       "          0.02438284, 0.02318673, 0.02313174, 0.02043386, 0.02124227,\n",
       "          0.01619996, 0.01543603, 0.01682793, 0.02052658, 0.02148737,\n",
       "          0.0189878 , 0.01966195, 0.01932485, 0.01800451, 0.0166788 ,\n",
       "          0.01611636, 0.02016733, 0.01868088, 0.01457435, 0.0140191 ,\n",
       "          0.0149571 , 0.0172877 , 0.01579395, 0.01944452, 0.0222198 ,\n",
       "          0.02117461, 0.0199758 , 0.0183888 , 0.01674464, 0.02121555,\n",
       "          0.02182576, 0.02847283, 0.02912275, 0.02668704, 0.02705292,\n",
       "          0.02662063, 0.02546324, 0.02550387, 0.02447971, 0.02170665,\n",
       "          0.02056893, 0.02236849, 0.02182058, 0.02883595, 0.03225546,\n",
       "          0.03408476, 0.02817891, 0.03130204, 0.02594129, 0.02917224,\n",
       "          0.02711495, 0.02159221, 0.02541433, 0.02432412, 0.02084452,\n",
       "          0.01942942, 0.02035847, 0.01692929, 0.01749149, 0.01579359,\n",
       "          0.01583408, 0.01733492, 0.01633102, 0.01481453, 0.01557633,\n",
       "          0.01626531, 0.01714901, 0.01498319, 0.01433597, 0.01477156,\n",
       "          0.01462558, 0.01313304, 0.01400219, 0.01258629, 0.01305994,\n",
       "          0.01871058, 0.01890393, 0.01672059, 0.01495781, 0.01515576,\n",
       "          0.01939267, 0.01983523, 0.02192491, 0.01776147, 0.01724716,\n",
       "          0.0170002 , 0.01518173, 0.01607487, 0.01591691, 0.01664112,\n",
       "          0.0179369 , 0.01562048, 0.01326604, 0.01442895, 0.0155031 ,\n",
       "          0.01860793, 0.02168147, 0.02039295, 0.01838854, 0.01825112,\n",
       "          0.01873587, 0.01474748, 0.01245581, 0.01364925, 0.01274676,\n",
       "          0.01124473, 0.01048891, 0.01068911, 0.01343083, 0.01212202,\n",
       "          0.01053276, 0.01181435, 0.01012357, 0.01073643, 0.01024914,\n",
       "          0.00907605, 0.01116141, 0.01348208, 0.01265958, 0.01323889,\n",
       "          0.01282548, 0.01319178, 0.01264043, 0.01317705, 0.01533971,\n",
       "          0.01453074, 0.01462479, 0.01426349, 0.0124744 , 0.01184779,\n",
       "          0.0117157 , 0.01199693, 0.01137218, 0.01190653, 0.01202383,\n",
       "          0.01100969, 0.01227152, 0.01211716, 0.01218649, 0.01423117,\n",
       "          0.01224045, 0.01287837, 0.01327247, 0.01807675, 0.01701538,\n",
       "          0.01993406, 0.01734176, 0.01827883, 0.0183436 , 0.01691767,\n",
       "          0.01568983, 0.01573894, 0.01921232, 0.01889914, 0.02062707,\n",
       "          0.01983231, 0.01995617, 0.01943974, 0.01753996, 0.02002511,\n",
       "          0.01759743, 0.01711238, 0.01483667, 0.01290234, 0.0112427 ,\n",
       "          0.00948463, 0.01285013, 0.01434244, 0.01517215, 0.01327596,\n",
       "          0.01284977, 0.01481889, 0.01249567, 0.01147913, 0.01447725,\n",
       "          0.01505432, 0.01446002, 0.0168143 , 0.02237059, 0.02183045,\n",
       "          0.02060291, 0.02528876, 0.02425973, 0.02213832, 0.02147304,\n",
       "          0.02185045, 0.02460854, 0.02266628, 0.02099042, 0.019144  ,\n",
       "          0.0200809 , 0.01661294, 0.01625095, 0.01707461, 0.02063614,\n",
       "          0.01784326, 0.01784595, 0.02120365, 0.02357865, 0.02674043,\n",
       "          0.02694967, 0.02485213, 0.02658407, 0.02921013, 0.02855051,\n",
       "          0.02674224, 0.02425992, 0.02058944, 0.01644379, 0.01334709,\n",
       "          0.0135724 , 0.01453537, 0.01333232, 0.01359727, 0.0118567 ,\n",
       "          0.0138138 , 0.01280793, 0.01252482, 0.01362872, 0.01272312,\n",
       "          0.01158399, 0.01168148, 0.00989829, 0.00835929, 0.00962142,\n",
       "          0.00942437, 0.0101618 , 0.01020153, 0.01001691, 0.01108533,\n",
       "          0.01102194, 0.01096601, 0.01094542, 0.00941319, 0.0097863 ,\n",
       "          0.00875056, 0.00902784, 0.00985114, 0.00903066, 0.00830854,\n",
       "          0.00784891, 0.01138987, 0.01119067, 0.01028154, 0.01049035,\n",
       "          0.01053218, 0.00973044, 0.01001683, 0.01238108, 0.01491865,\n",
       "          0.01525883, 0.01276032, 0.01325565, 0.01243792, 0.01177745,\n",
       "          0.01186526, 0.01152254, 0.01307019, 0.01486053, 0.01282718,\n",
       "          0.01165738, 0.01174655, 0.0112116 , 0.01134032, 0.01236408,\n",
       "          0.01174156, 0.01229184, 0.0115815 , 0.01513497, 0.01491179,\n",
       "          0.01391146, 0.01462703, 0.01438968, 0.01211235, 0.01218251,\n",
       "          0.01187255, 0.01426173, 0.01825976, 0.01465112, 0.01415481,\n",
       "          0.01544797, 0.01805785, 0.01758449, 0.01667258, 0.01561639,\n",
       "          0.01569118, 0.01470011, 0.01443064, 0.01335881, 0.01261811,\n",
       "          0.01267979, 0.01553787, 0.01483253, 0.01463559, 0.01576966,\n",
       "          0.01685931, 0.01365715, 0.01282179, 0.01425679, 0.01513573,\n",
       "          0.01452352, 0.01241449, 0.01075467, 0.0108145 , 0.01051006,\n",
       "          0.01090095, 0.01182402, 0.01065619, 0.01113822, 0.011681  ,\n",
       "          0.01407579, 0.0141281 , 0.01969314, 0.01877359, 0.01747946,\n",
       "          0.01664676, 0.01830588, 0.01652588, 0.02202542, 0.02542798,\n",
       "          0.02493611, 0.02516157, 0.02429168, 0.02612951, 0.02289627,\n",
       "          0.02080212, 0.02035025, 0.02010984, 0.01737432, 0.01606469,\n",
       "          0.01907643, 0.01888315, 0.01713995, 0.01504584, 0.01523909,\n",
       "          0.01409435, 0.01292641, 0.01279532, 0.01244404, 0.01455101,\n",
       "          0.01282629, 0.01092753, 0.01080875, 0.00997915, 0.01133395,\n",
       "          0.01148013, 0.01187886, 0.01316574, 0.01160247, 0.0131767 ,\n",
       "          0.01624366, 0.01755153, 0.01444734, 0.0151369 , 0.01505375,\n",
       "          0.01311566, 0.01219415, 0.01073173, 0.01205113, 0.01271417,\n",
       "          0.01027538, 0.01000452, 0.01094935, 0.01051126, 0.01008054,\n",
       "          0.01213796, 0.01257783, 0.01312541, 0.01237132, 0.01140859,\n",
       "          0.01117634, 0.01115027, 0.01072349, 0.01074266, 0.01082141,\n",
       "          0.01091243, 0.01046602, 0.00993612, 0.01053245, 0.01296867,\n",
       "          0.01229652, 0.01236908, 0.01270389, 0.01645214, 0.01910109,\n",
       "          0.02088573, 0.01818924, 0.01820872, 0.02533747, 0.02591738,\n",
       "          0.02310817, 0.02351514, 0.02551036, 0.02103218, 0.02387758,\n",
       "          0.03114223, 0.05018834, 0.03800827, 0.03806791, 0.03154166,\n",
       "          0.02782845, 0.02577811, 0.02181341, 0.01919753, 0.0153885 ,\n",
       "          0.01296562, 0.00999663, 0.01260638, 0.01169073, 0.01488251,\n",
       "          0.01351065, 0.01404858, 0.01349435, 0.01655291, 0.0140794 ,\n",
       "          0.01323488, 0.02360136, 0.02474983, 0.02280376, 0.02807843,\n",
       "          0.0246431 , 0.02267475, 0.02007729, 0.01776425, 0.01552347,\n",
       "          0.0167593 , 0.01545734, 0.01672962, 0.01412778, 0.012255  ,\n",
       "          0.01311932, 0.01227646, 0.01254699, 0.01302875, 0.01552643,\n",
       "          0.01544941, 0.02053081, 0.01926615, 0.0214633 , 0.01930572,\n",
       "          0.02503589, 0.02327101, 0.02224072, 0.02270431, 0.02177047,\n",
       "          0.0205014 , 0.02192366, 0.01982991, 0.01967651, 0.01853755,\n",
       "          0.01894568, 0.01881362, 0.02128024, 0.02123079, 0.02266661,\n",
       "          0.02212862, 0.02093879, 0.0216111 , 0.02812398, 0.02619405,\n",
       "          0.02775624, 0.0259559 , 0.01901443, 0.01657265, 0.0159664 ,\n",
       "          0.01578271, 0.01605055, 0.01406916, 0.01404341, 0.01908601,\n",
       "          0.01841714, 0.01956309, 0.02094653, 0.01996971, 0.01644544,\n",
       "          0.01584904, 0.01395541, 0.01413452, 0.01212718, 0.01013821,\n",
       "          0.01028543, 0.01097031, 0.01242172, 0.01165504, 0.01470192,\n",
       "          0.01534534, 0.01368666, 0.01383148, 0.01452885, 0.01543176,\n",
       "          0.01743283, 0.03698351, 0.03087496, 0.02431482, 0.02210275,\n",
       "          0.01723487, 0.01573567, 0.01845381, 0.02287882, 0.02320324,\n",
       "          0.02435438, 0.01866891, 0.01730273, 0.02073175, 0.02052309,\n",
       "          0.02508063, 0.02535205, 0.02381861, 0.0185102 , 0.01902351,\n",
       "          0.01603688, 0.01448268, 0.01413291, 0.01319642, 0.01340913,\n",
       "          0.01953693, 0.01703471, 0.01792104, 0.01672307, 0.01770107,\n",
       "          0.02101425, 0.01940096, 0.01628681, 0.01531731, 0.01727991,\n",
       "          0.01365418, 0.01424156, 0.01386627, 0.01342951, 0.01162333,\n",
       "          0.01210798, 0.01239199, 0.01365409, 0.02095947, 0.02189244,\n",
       "          0.02365658, 0.02304475, 0.02695317, 0.02487472, 0.03038651,\n",
       "          0.03231872, 0.03056134, 0.03482258, 0.03191787, 0.03948212,\n",
       "          0.0384416 , 0.03538253, 0.03585012, 0.03171818, 0.02668073,\n",
       "          0.02836623, 0.02663468, 0.02453736, 0.0241827 , 0.01880243,\n",
       "          0.01671901, 0.02037078, 0.0199577 , 0.02752283, 0.02867448,\n",
       "          0.02638086, 0.02568167, 0.04349414, 0.06810625, 0.0708537 ,\n",
       "          0.0768826 , 0.05802489, 0.06623739, 0.05780061, 0.05071262,\n",
       "          0.04960608, 0.05433032, 0.04344426, 0.04696514, 0.04335061,\n",
       "          0.03866114, 0.03466423, 0.02871301, 0.02772507, 0.0267553 ,\n",
       "          0.02665883, 0.02464762, 0.02109616, 0.02030569, 0.02354162,\n",
       "          0.02237612, 0.02290754, 0.02231376, 0.01798653, 0.01722357,\n",
       "          0.0174467 , 0.01613623, 0.01458335, 0.01337986, 0.01323518,\n",
       "          0.0197938 , 0.01985789, 0.02301893, 0.01981707, 0.02039489,\n",
       "          0.01889933, 0.01821354, 0.01735085, 0.01561205, 0.01499867,\n",
       "          0.01523618, 0.01217193, 0.01267872, 0.01382826, 0.01453728,\n",
       "          0.01545742, 0.02032378, 0.01790095, 0.02282078, 0.02052563,\n",
       "          0.02034909, 0.01875268, 0.01559244, 0.01399916, 0.01329688,\n",
       "          0.0121091 , 0.01232183, 0.01252732, 0.01084048, 0.00989907,\n",
       "          0.01303137, 0.01251921, 0.01249566, 0.01214099, 0.01346933,\n",
       "          0.01427299, 0.01402085, 0.01366985, 0.01331349, 0.01320276,\n",
       "          0.01302786, 0.012718  , 0.01130661, 0.01143847, 0.0109277 ,\n",
       "          0.01120509, 0.012692  , 0.01167696, 0.01460295, 0.02315908,\n",
       "          0.01862521, 0.01895469, 0.01674908, 0.01667569, 0.01249376,\n",
       "          0.01446044, 0.01059126, 0.01015985, 0.01049852, 0.00992928,\n",
       "          0.01102814, 0.01215819, 0.0129664 , 0.01611065, 0.01231991,\n",
       "          0.01369335, 0.01297424, 0.0132027 , 0.01206775, 0.01370186,\n",
       "          0.01711948, 0.0154108 , 0.01337545, 0.0132545 , 0.01319678,\n",
       "          0.01188915, 0.01341137, 0.01232481, 0.01176498, 0.01281596,\n",
       "          0.0132069 , 0.01262835, 0.01207979, 0.01150268, 0.01293243,\n",
       "          0.01398253, 0.0134451 , 0.01533369, 0.0129731 , 0.01435264,\n",
       "          0.01355388, 0.01300749, 0.01357796, 0.01348033, 0.01283598,\n",
       "          0.01424567, 0.01278889, 0.01255345, 0.02449642, 0.02068768,\n",
       "          0.02326223, 0.02294144, 0.02906301, 0.02087939, 0.01773927,\n",
       "          0.01704637, 0.01883629, 0.01563251, 0.01134179, 0.00991182,\n",
       "          0.01041005, 0.01113519, 0.01297217, 0.01356173, 0.01359884,\n",
       "          0.01836848, 0.01651622, 0.02022078, 0.01976226, 0.0181166 ,\n",
       "          0.01683584, 0.01720158, 0.02129331, 0.0214417 , 0.02630312,\n",
       "          0.02934218, 0.02814101, 0.03184635, 0.02813005, 0.02476422,\n",
       "          0.02096929, 0.01881249, 0.01417567]),\n",
       "   'test': array([0.01571153, 0.01412575, 0.01217576, 0.01221791, 0.01045435,\n",
       "          0.01328657, 0.01478478, 0.01308768, 0.01170977, 0.01481026,\n",
       "          0.01572183, 0.0158405 , 0.01911643, 0.01792311, 0.01429866,\n",
       "          0.01230938, 0.01169336, 0.00983322, 0.01025796, 0.01171871,\n",
       "          0.01197795, 0.01369185, 0.01199588, 0.01283859, 0.01243281,\n",
       "          0.01348101, 0.01388811, 0.01388811, 0.01245121, 0.0124512 ])}},\n",
       " 'lasso': {'cv': {'family': 'lasso',\n",
       "   'params': {'model__alpha': 0.001, 'model__max_iter': 5000},\n",
       "   'cv_rmse_mean': 0.009542583566628102,\n",
       "   'cv_rmse_std': 0.004052062153601183},\n",
       "  'metrics': {'name': 'lasso_tuned',\n",
       "   'train': {'rmse': 0.009726997294809265,\n",
       "    'mae': 0.006110056959430841,\n",
       "    'r2': 0.5863933001895658},\n",
       "   'val': {'rmse': 0.009395058379280412,\n",
       "    'mae': 0.0056636981429092625,\n",
       "    'r2': 0.41433104016959477},\n",
       "   'test': {'rmse': 0.004834203937181804,\n",
       "    'mae': 0.003966779363278979,\n",
       "    'r2': -0.7246847431064485}},\n",
       "  'model': Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                  ('model', Lasso(alpha=0.001, max_iter=5000))]),\n",
       "  'preds': {'train': array([0.01640272, 0.01603738, 0.01540305, ..., 0.03477897, 0.03651579,\n",
       "          0.03890819]),\n",
       "   'val': array([0.03974469, 0.040262  , 0.04127804, 0.03772918, 0.03792498,\n",
       "          0.03745436, 0.03691153, 0.03742124, 0.03631462, 0.03270783,\n",
       "          0.03132438, 0.02895631, 0.02826227, 0.02956552, 0.02545558,\n",
       "          0.02709189, 0.02782856, 0.03025139, 0.03016831, 0.02874749,\n",
       "          0.02831788, 0.02966904, 0.03069778, 0.02805834, 0.02673693,\n",
       "          0.02877367, 0.02901567, 0.02736739, 0.02371303, 0.02276103,\n",
       "          0.0224546 , 0.02121115, 0.01823151, 0.01890209, 0.02327424,\n",
       "          0.02252052, 0.02143695, 0.01990441, 0.01870394, 0.02265692,\n",
       "          0.0256221 , 0.02557704, 0.02503517, 0.02644347, 0.02717959,\n",
       "          0.02331973, 0.02306913, 0.02788869, 0.02846501, 0.02778206,\n",
       "          0.02582246, 0.0235267 , 0.02620885, 0.02377295, 0.02538708,\n",
       "          0.02773077, 0.02609935, 0.0243252 , 0.02575209, 0.02392711,\n",
       "          0.02522137, 0.02353326, 0.02368313, 0.02194498, 0.02206975,\n",
       "          0.0179689 , 0.01686145, 0.01828295, 0.02098985, 0.02186821,\n",
       "          0.02035098, 0.02058821, 0.02011199, 0.01952774, 0.01890317,\n",
       "          0.01805154, 0.02120727, 0.02027854, 0.01709216, 0.01705158,\n",
       "          0.01803799, 0.02007828, 0.01847923, 0.02034172, 0.02264452,\n",
       "          0.02190595, 0.02077017, 0.01930333, 0.018171  , 0.02239381,\n",
       "          0.02193412, 0.02752957, 0.02772462, 0.02574449, 0.0262322 ,\n",
       "          0.02561006, 0.0248442 , 0.02499263, 0.02380309, 0.02108079,\n",
       "          0.02106724, 0.02271346, 0.02206262, 0.02796265, 0.03014612,\n",
       "          0.0320643 , 0.0279452 , 0.03030246, 0.02640395, 0.02943629,\n",
       "          0.02797446, 0.02348939, 0.02617369, 0.02504664, 0.02247829,\n",
       "          0.02119362, 0.02099184, 0.01862455, 0.01877935, 0.01738398,\n",
       "          0.01761481, 0.01882272, 0.01760657, 0.01658934, 0.01702682,\n",
       "          0.01731007, 0.01738845, 0.01616583, 0.01551849, 0.01578755,\n",
       "          0.01553832, 0.01427459, 0.01486561, 0.01428813, 0.01445531,\n",
       "          0.0194508 , 0.01986027, 0.01775535, 0.01559367, 0.01637489,\n",
       "          0.01923219, 0.02030496, 0.02182295, 0.01797986, 0.01765779,\n",
       "          0.0176944 , 0.01598311, 0.01660311, 0.01612375, 0.01644395,\n",
       "          0.01752059, 0.01564909, 0.01409566, 0.01543887, 0.01588576,\n",
       "          0.01854545, 0.02096595, 0.01947051, 0.01781898, 0.01732939,\n",
       "          0.01783199, 0.01472005, 0.01347604, 0.01367737, 0.01287553,\n",
       "          0.01221285, 0.01109272, 0.01091156, 0.01338256, 0.01246283,\n",
       "          0.01084753, 0.01207325, 0.01122341, 0.01158332, 0.011034  ,\n",
       "          0.01033024, 0.01182171, 0.0138536 , 0.01343791, 0.01348807,\n",
       "          0.01328454, 0.01345282, 0.01308464, 0.01343847, 0.01552962,\n",
       "          0.01493804, 0.01441404, 0.0138327 , 0.01223284, 0.01196172,\n",
       "          0.01162121, 0.01188996, 0.01144326, 0.0115868 , 0.01206224,\n",
       "          0.0113931 , 0.01208612, 0.01189717, 0.01122736, 0.01364894,\n",
       "          0.01203933, 0.01266251, 0.01343425, 0.01807803, 0.01766597,\n",
       "          0.01922025, 0.01747349, 0.01796667, 0.01797802, 0.01723708,\n",
       "          0.01608079, 0.01581835, 0.01918965, 0.01912092, 0.0209952 ,\n",
       "          0.02020354, 0.02032793, 0.01954529, 0.0175878 , 0.02061783,\n",
       "          0.01844815, 0.01775998, 0.01593522, 0.01432009, 0.01304952,\n",
       "          0.01199886, 0.01406318, 0.01448641, 0.01501205, 0.01333921,\n",
       "          0.01343048, 0.01479628, 0.01349459, 0.01260488, 0.01544183,\n",
       "          0.01574347, 0.01580647, 0.01782332, 0.02286341, 0.02214012,\n",
       "          0.02171075, 0.02519474, 0.02433472, 0.02266113, 0.02231542,\n",
       "          0.0224248 , 0.02530999, 0.02321805, 0.02200063, 0.02040637,\n",
       "          0.020642  , 0.01811798, 0.01671266, 0.01824506, 0.0205431 ,\n",
       "          0.01810383, 0.01799148, 0.02118498, 0.02401759, 0.02601274,\n",
       "          0.02502456, 0.02338403, 0.02568418, 0.0270642 , 0.02713622,\n",
       "          0.02577475, 0.02373057, 0.02055791, 0.01773205, 0.01566972,\n",
       "          0.01548287, 0.01556141, 0.0141256 , 0.01437421, 0.0123281 ,\n",
       "          0.01401515, 0.01307398, 0.01333622, 0.01406339, 0.01315352,\n",
       "          0.01231288, 0.01173382, 0.01042095, 0.00947143, 0.0103191 ,\n",
       "          0.01006144, 0.01066543, 0.01057448, 0.01014499, 0.01145377,\n",
       "          0.01131068, 0.01168915, 0.01194908, 0.01064926, 0.01094262,\n",
       "          0.00997258, 0.01054397, 0.01098681, 0.01040106, 0.01011721,\n",
       "          0.00961904, 0.01230122, 0.01253734, 0.01153176, 0.01148235,\n",
       "          0.01100877, 0.01074276, 0.01065669, 0.01273738, 0.01508307,\n",
       "          0.01581049, 0.0137356 , 0.01379741, 0.01346681, 0.01315154,\n",
       "          0.01286668, 0.01300121, 0.01402457, 0.01556342, 0.0141088 ,\n",
       "          0.01286012, 0.01287965, 0.01224226, 0.01271866, 0.01311035,\n",
       "          0.0122877 , 0.01276383, 0.01208371, 0.01544796, 0.01487956,\n",
       "          0.01471178, 0.01503367, 0.01411133, 0.0127616 , 0.01242728,\n",
       "          0.01198789, 0.01369133, 0.01806365, 0.01463667, 0.01421337,\n",
       "          0.01542422, 0.01750482, 0.01692239, 0.01621515, 0.01516862,\n",
       "          0.01562679, 0.01513391, 0.01488117, 0.01304401, 0.01277434,\n",
       "          0.01300851, 0.01518846, 0.01496667, 0.01511495, 0.0157591 ,\n",
       "          0.01642552, 0.01395035, 0.01380956, 0.01468361, 0.0152793 ,\n",
       "          0.01497265, 0.01323732, 0.01218237, 0.01227473, 0.01203935,\n",
       "          0.01234329, 0.01297274, 0.01174267, 0.01194524, 0.01270811,\n",
       "          0.01492851, 0.01461629, 0.01905028, 0.01831734, 0.01717956,\n",
       "          0.01651741, 0.01856323, 0.01684429, 0.02116954, 0.02457961,\n",
       "          0.02366847, 0.02371215, 0.02288906, 0.02423538, 0.02174091,\n",
       "          0.02007985, 0.01975727, 0.01975266, 0.01769646, 0.01615364,\n",
       "          0.01919987, 0.01869265, 0.01724059, 0.01470198, 0.01491065,\n",
       "          0.01405625, 0.01347852, 0.01241433, 0.01197338, 0.01362243,\n",
       "          0.01227663, 0.01085591, 0.01103595, 0.01058012, 0.0110686 ,\n",
       "          0.01054986, 0.01064787, 0.01245095, 0.01105026, 0.01259644,\n",
       "          0.01551491, 0.0167192 , 0.01399076, 0.0142076 , 0.01423928,\n",
       "          0.01322082, 0.01276312, 0.01160576, 0.0126604 , 0.01297527,\n",
       "          0.01062123, 0.0098989 , 0.01092038, 0.01113237, 0.01015215,\n",
       "          0.01180772, 0.01203069, 0.01247656, 0.01153255, 0.01106228,\n",
       "          0.01074481, 0.01156184, 0.01133506, 0.01125539, 0.01138187,\n",
       "          0.01164265, 0.01123233, 0.01119194, 0.0118656 , 0.0128803 ,\n",
       "          0.01199841, 0.01248437, 0.01248847, 0.01612086, 0.01879761,\n",
       "          0.02010234, 0.01784552, 0.01768999, 0.02442419, 0.02472282,\n",
       "          0.02208389, 0.02252236, 0.02430101, 0.02041819, 0.0234131 ,\n",
       "          0.02947563, 0.04752294, 0.0369571 , 0.03611699, 0.03078808,\n",
       "          0.02731968, 0.0259262 , 0.02218597, 0.02016929, 0.01674428,\n",
       "          0.01468619, 0.01227102, 0.01412751, 0.0139262 , 0.01630322,\n",
       "          0.01418426, 0.01472629, 0.01420697, 0.0170239 , 0.01520801,\n",
       "          0.01395511, 0.02365094, 0.02445141, 0.02327378, 0.02751174,\n",
       "          0.02413798, 0.02237872, 0.02045427, 0.01866596, 0.01684958,\n",
       "          0.01746542, 0.01668415, 0.01746318, 0.01500926, 0.01382115,\n",
       "          0.01383489, 0.01309381, 0.01350917, 0.01264506, 0.01496303,\n",
       "          0.0148041 , 0.01895181, 0.0183644 , 0.02058454, 0.01891157,\n",
       "          0.0240606 , 0.02250223, 0.02181701, 0.02215405, 0.02118055,\n",
       "          0.02011592, 0.02121644, 0.01957402, 0.01918193, 0.01791769,\n",
       "          0.01787912, 0.0175584 , 0.02004011, 0.01973726, 0.0210769 ,\n",
       "          0.02093803, 0.01973002, 0.02054646, 0.02697548, 0.02553682,\n",
       "          0.02601488, 0.02439651, 0.01880432, 0.01646288, 0.01587935,\n",
       "          0.01605967, 0.01579339, 0.01302433, 0.01271917, 0.01711387,\n",
       "          0.01634299, 0.01744309, 0.01878866, 0.01815397, 0.01512044,\n",
       "          0.01411825, 0.01311335, 0.01333229, 0.0117374 , 0.0102868 ,\n",
       "          0.01041029, 0.01057639, 0.01073322, 0.00919141, 0.01225915,\n",
       "          0.01281317, 0.01147204, 0.01205017, 0.01224489, 0.01333389,\n",
       "          0.01551746, 0.0345208 , 0.02958588, 0.02317359, 0.0210839 ,\n",
       "          0.01682212, 0.0159699 , 0.01855301, 0.02146499, 0.02181265,\n",
       "          0.02230667, 0.01764954, 0.01687131, 0.02030214, 0.01999322,\n",
       "          0.02399192, 0.02357627, 0.0224083 , 0.0180028 , 0.01879977,\n",
       "          0.01651167, 0.01532215, 0.01499543, 0.01371242, 0.01363921,\n",
       "          0.01877619, 0.01643985, 0.01709801, 0.01611112, 0.01705845,\n",
       "          0.02023445, 0.01851053, 0.01637778, 0.01577103, 0.01788708,\n",
       "          0.01539757, 0.01591186, 0.01566779, 0.01459254, 0.01327158,\n",
       "          0.01388829, 0.01378157, 0.01500529, 0.02084189, 0.0217105 ,\n",
       "          0.02298314, 0.02272049, 0.02656083, 0.02419893, 0.02956331,\n",
       "          0.03142996, 0.02945471, 0.0336242 , 0.03071344, 0.03749499,\n",
       "          0.03669616, 0.03367281, 0.03383301, 0.03083577, 0.0267564 ,\n",
       "          0.02835984, 0.02630851, 0.0250059 , 0.02388032, 0.01975385,\n",
       "          0.01816236, 0.02092937, 0.02086046, 0.02721242, 0.02767895,\n",
       "          0.02619826, 0.02577049, 0.04181141, 0.06311763, 0.06559657,\n",
       "          0.07104238, 0.05425677, 0.06271879, 0.05576136, 0.04935701,\n",
       "          0.04873468, 0.04847177, 0.03980937, 0.04272676, 0.03893315,\n",
       "          0.03532671, 0.03367507, 0.02868225, 0.02851479, 0.02673783,\n",
       "          0.02685407, 0.02410766, 0.02175597, 0.02164939, 0.02424285,\n",
       "          0.02313587, 0.02274273, 0.02180671, 0.01917069, 0.01871166,\n",
       "          0.0189063 , 0.01761344, 0.01655913, 0.01515921, 0.01486476,\n",
       "          0.0207186 , 0.0203532 , 0.02304086, 0.01992626, 0.02096823,\n",
       "          0.01988058, 0.01933481, 0.01904358, 0.01687884, 0.01638322,\n",
       "          0.01699   , 0.01469471, 0.01490158, 0.01496728, 0.01517171,\n",
       "          0.01608923, 0.02041245, 0.01863479, 0.0225209 , 0.02082849,\n",
       "          0.02107714, 0.01983165, 0.01687285, 0.01585259, 0.01535616,\n",
       "          0.01476619, 0.01453075, 0.01468273, 0.01385102, 0.01316737,\n",
       "          0.01553226, 0.01467281, 0.01360308, 0.013244  , 0.01442697,\n",
       "          0.01522177, 0.01593218, 0.0151945 , 0.01434391, 0.01444529,\n",
       "          0.014416  , 0.01395616, 0.01293684, 0.01295045, 0.01195642,\n",
       "          0.01246853, 0.01379349, 0.01308823, 0.01536895, 0.02267614,\n",
       "          0.01892433, 0.01955554, 0.01800971, 0.01777033, 0.01457908,\n",
       "          0.01586892, 0.01323771, 0.01240188, 0.01279051, 0.0121841 ,\n",
       "          0.01256283, 0.01313976, 0.01367887, 0.0155801 , 0.01277788,\n",
       "          0.01403566, 0.01375636, 0.01417536, 0.01326336, 0.0142809 ,\n",
       "          0.01757837, 0.01587547, 0.01395777, 0.0137144 , 0.0137885 ,\n",
       "          0.01300766, 0.01347577, 0.01237718, 0.01255061, 0.01348423,\n",
       "          0.01409711, 0.01358459, 0.01324007, 0.01249875, 0.01326278,\n",
       "          0.01449784, 0.01432083, 0.01578371, 0.01369732, 0.01495709,\n",
       "          0.01477574, 0.01454678, 0.01505774, 0.01490645, 0.01438199,\n",
       "          0.01549815, 0.01383874, 0.01403762, 0.0253373 , 0.02211931,\n",
       "          0.02436917, 0.02402715, 0.02960678, 0.02277124, 0.0192223 ,\n",
       "          0.018649  , 0.0203689 , 0.01795396, 0.01442722, 0.01357879,\n",
       "          0.01409108, 0.01482127, 0.01556626, 0.01600369, 0.01548703,\n",
       "          0.01952525, 0.01792356, 0.02091404, 0.02044691, 0.01880639,\n",
       "          0.01776668, 0.01806102, 0.02226086, 0.0223864 , 0.02593345,\n",
       "          0.02860133, 0.02736673, 0.0306472 , 0.02719786, 0.02407264,\n",
       "          0.02112942, 0.01892874, 0.01567638]),\n",
       "   'test': array([0.01712082, 0.01514523, 0.01329559, 0.01286153, 0.01106344,\n",
       "          0.01338552, 0.01422826, 0.01279533, 0.01128488, 0.01466279,\n",
       "          0.01556523, 0.01580921, 0.01903946, 0.01784005, 0.01452679,\n",
       "          0.0127927 , 0.01228298, 0.01078684, 0.01110633, 0.01187004,\n",
       "          0.01205843, 0.01369096, 0.01210651, 0.01266361, 0.01252544,\n",
       "          0.01386184, 0.0140806 , 0.0140806 , 0.01261732, 0.01261732])}},\n",
       " 'elasticnet': {'cv': {'family': 'elasticnet',\n",
       "   'params': {'model__alpha': 0.001,\n",
       "    'model__l1_ratio': 0.5,\n",
       "    'model__max_iter': 5000},\n",
       "   'cv_rmse_mean': 0.009517987347203662,\n",
       "   'cv_rmse_std': 0.00375919981932936},\n",
       "  'metrics': {'name': 'elasticnet_tuned',\n",
       "   'train': {'rmse': 0.00965799207044481,\n",
       "    'mae': 0.006100402183654794,\n",
       "    'r2': 0.5922408982617042},\n",
       "   'val': {'rmse': 0.009377141401662209,\n",
       "    'mae': 0.00571446503899148,\n",
       "    'r2': 0.4165627265332603},\n",
       "   'test': {'rmse': 0.004653837661685638,\n",
       "    'mae': 0.0038258631611107897,\n",
       "    'r2': -0.5983881302123444}},\n",
       "  'model': Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                  ('model', ElasticNet(alpha=0.001, max_iter=5000))]),\n",
       "  'preds': {'train': array([0.01617914, 0.01566845, 0.01487158, ..., 0.03495343, 0.03661614,\n",
       "          0.03924626]),\n",
       "   'val': array([0.0407052 , 0.04195334, 0.04296489, 0.03867483, 0.03886521,\n",
       "          0.037808  , 0.03682498, 0.03738581, 0.03686789, 0.032337  ,\n",
       "          0.03124075, 0.02877117, 0.02809329, 0.02926374, 0.02503992,\n",
       "          0.02693364, 0.02807546, 0.0308252 , 0.03079814, 0.02946199,\n",
       "          0.02874947, 0.02993071, 0.03105441, 0.02753534, 0.02621146,\n",
       "          0.02863012, 0.02884337, 0.02685537, 0.02453181, 0.02371317,\n",
       "          0.02336516, 0.02201926, 0.01862519, 0.01843015, 0.02324878,\n",
       "          0.02239809, 0.02118209, 0.01965732, 0.01840722, 0.02261762,\n",
       "          0.02589314, 0.02655099, 0.0258547 , 0.02743961, 0.0277518 ,\n",
       "          0.02351043, 0.02302755, 0.02849835, 0.02916448, 0.02873545,\n",
       "          0.02673614, 0.02409405, 0.02685962, 0.02409794, 0.02566291,\n",
       "          0.02828088, 0.02664612, 0.0244577 , 0.0260416 , 0.02395516,\n",
       "          0.02531868, 0.02357613, 0.02361549, 0.02166078, 0.02176527,\n",
       "          0.0172667 , 0.01650421, 0.01798704, 0.02109915, 0.02234366,\n",
       "          0.02052138, 0.02051581, 0.02001825, 0.0192329 , 0.01829015,\n",
       "          0.01778677, 0.02147823, 0.02028515, 0.01670269, 0.01659157,\n",
       "          0.01768461, 0.0197169 , 0.01818489, 0.02045536, 0.02322872,\n",
       "          0.02230771, 0.02083689, 0.01952172, 0.01814861, 0.02252707,\n",
       "          0.02207981, 0.02838409, 0.02856333, 0.0264507 , 0.02692224,\n",
       "          0.02621851, 0.02530237, 0.02549607, 0.02425929, 0.021007  ,\n",
       "          0.02103039, 0.02270951, 0.02197442, 0.02846429, 0.03114116,\n",
       "          0.03313222, 0.02851441, 0.03100874, 0.02659359, 0.02959999,\n",
       "          0.02796766, 0.02340599, 0.02634084, 0.02536384, 0.02239048,\n",
       "          0.02123326, 0.02124319, 0.01843552, 0.01845843, 0.01687143,\n",
       "          0.01707388, 0.01829118, 0.01721362, 0.01620049, 0.01696848,\n",
       "          0.01733615, 0.0174236 , 0.01578597, 0.01520457, 0.01533937,\n",
       "          0.01506194, 0.01365715, 0.01466593, 0.01401274, 0.0142513 ,\n",
       "          0.01965301, 0.02013013, 0.01758041, 0.01523176, 0.01608619,\n",
       "          0.01907099, 0.02021339, 0.02229818, 0.01821507, 0.01785953,\n",
       "          0.01772839, 0.01578527, 0.01622774, 0.01620992, 0.01651247,\n",
       "          0.01757737, 0.01550033, 0.01367841, 0.01488704, 0.015401  ,\n",
       "          0.01824062, 0.02109219, 0.01963558, 0.01773967, 0.017229  ,\n",
       "          0.01768962, 0.01416686, 0.01277431, 0.01329381, 0.01245328,\n",
       "          0.01166342, 0.01057048, 0.01068226, 0.01299591, 0.01195262,\n",
       "          0.01017342, 0.0113271 , 0.0103983 , 0.01100569, 0.01062311,\n",
       "          0.00984893, 0.01177269, 0.01393002, 0.01326225, 0.01324347,\n",
       "          0.0130278 , 0.01288106, 0.01242315, 0.01302277, 0.01528503,\n",
       "          0.01476065, 0.01442698, 0.01373879, 0.01186133, 0.01137706,\n",
       "          0.0109838 , 0.01116287, 0.01070158, 0.01096613, 0.01166919,\n",
       "          0.01095897, 0.0116928 , 0.01158657, 0.01088445, 0.01338491,\n",
       "          0.01156988, 0.01227811, 0.01312292, 0.01816446, 0.01766692,\n",
       "          0.01952993, 0.01763135, 0.01815502, 0.01816578, 0.01731859,\n",
       "          0.01601567, 0.01586104, 0.0194526 , 0.0192746 , 0.02131649,\n",
       "          0.0204763 , 0.02059908, 0.0197084 , 0.01741938, 0.02067651,\n",
       "          0.0183491 , 0.01766673, 0.01548216, 0.01391894, 0.01245525,\n",
       "          0.01145365, 0.01377762, 0.01464632, 0.01525216, 0.0133524 ,\n",
       "          0.01335608, 0.01478502, 0.01318116, 0.01209343, 0.01525482,\n",
       "          0.0156829 , 0.01570471, 0.01792305, 0.02358747, 0.02273354,\n",
       "          0.02222297, 0.02599153, 0.0250079 , 0.02304456, 0.02260598,\n",
       "          0.02279624, 0.02581835, 0.02351153, 0.0222027 , 0.02036064,\n",
       "          0.02051297, 0.01757652, 0.01620565, 0.01786478, 0.02051137,\n",
       "          0.0179774 , 0.01783783, 0.02136647, 0.02431484, 0.02644548,\n",
       "          0.02565979, 0.02386567, 0.02633876, 0.02782805, 0.02780163,\n",
       "          0.02619821, 0.02413996, 0.02049115, 0.01712938, 0.01474874,\n",
       "          0.01476093, 0.01490292, 0.0135145 , 0.01426224, 0.01205401,\n",
       "          0.01383551, 0.01261845, 0.01283151, 0.01334217, 0.01265822,\n",
       "          0.0116369 , 0.01158085, 0.01015469, 0.00913851, 0.00994254,\n",
       "          0.00986154, 0.01018764, 0.01011442, 0.00957628, 0.01099493,\n",
       "          0.01080263, 0.0112371 , 0.01148746, 0.01018006, 0.01038589,\n",
       "          0.00929466, 0.00967148, 0.0102255 , 0.00966277, 0.00932021,\n",
       "          0.00877844, 0.01206088, 0.01222605, 0.01111287, 0.01109735,\n",
       "          0.01068872, 0.01021559, 0.01039269, 0.0126759 , 0.01527175,\n",
       "          0.01608717, 0.01376616, 0.01358135, 0.01319204, 0.01270229,\n",
       "          0.01237054, 0.01252075, 0.01399558, 0.01561187, 0.01403449,\n",
       "          0.01252186, 0.01252326, 0.0116568 , 0.01206222, 0.01255174,\n",
       "          0.01202305, 0.01240936, 0.01174355, 0.01545204, 0.01475949,\n",
       "          0.01434138, 0.01488075, 0.01387516, 0.0121949 , 0.01210903,\n",
       "          0.0117762 , 0.01349094, 0.01824599, 0.01457646, 0.01397071,\n",
       "          0.01536323, 0.01760027, 0.01687628, 0.01598843, 0.01501744,\n",
       "          0.01546997, 0.01487364, 0.0145374 , 0.01310352, 0.01261373,\n",
       "          0.01281534, 0.01520977, 0.01488733, 0.01474253, 0.01567587,\n",
       "          0.0163647 , 0.01353876, 0.01349364, 0.01463322, 0.0152216 ,\n",
       "          0.01480971, 0.01309599, 0.01180464, 0.01178242, 0.01145941,\n",
       "          0.01192529, 0.01264395, 0.0114417 , 0.01171207, 0.01247659,\n",
       "          0.01481512, 0.01442288, 0.01933192, 0.01842999, 0.01721549,\n",
       "          0.01642807, 0.01872281, 0.01668778, 0.02157442, 0.02528132,\n",
       "          0.02437766, 0.02439924, 0.02363843, 0.02498449, 0.02219217,\n",
       "          0.02027966, 0.01982246, 0.01983588, 0.01735908, 0.01579188,\n",
       "          0.01943129, 0.01891615, 0.01722315, 0.01456535, 0.01469049,\n",
       "          0.01358295, 0.01292215, 0.01191072, 0.01166064, 0.01364825,\n",
       "          0.01203823, 0.01031446, 0.01058309, 0.01000959, 0.01043459,\n",
       "          0.00994326, 0.01041327, 0.01231398, 0.01078196, 0.01241326,\n",
       "          0.01557497, 0.01677522, 0.01374724, 0.01409587, 0.01412906,\n",
       "          0.01282053, 0.01225797, 0.0112153 , 0.01226576, 0.01258384,\n",
       "          0.0102308 , 0.00941017, 0.01037793, 0.01052904, 0.00949303,\n",
       "          0.01132356, 0.01161852, 0.01213266, 0.01128223, 0.01081499,\n",
       "          0.01036576, 0.01124535, 0.01096313, 0.0107868 , 0.01086659,\n",
       "          0.01107284, 0.01057572, 0.01053814, 0.01122887, 0.01249036,\n",
       "          0.01160069, 0.01207094, 0.01199652, 0.01617173, 0.01896634,\n",
       "          0.02050135, 0.01807627, 0.01803705, 0.0251828 , 0.0254315 ,\n",
       "          0.02246983, 0.02311146, 0.0249847 , 0.02050517, 0.02366542,\n",
       "          0.03031766, 0.04918444, 0.0382417 , 0.03760597, 0.03166295,\n",
       "          0.02784193, 0.02615   , 0.02220721, 0.01993305, 0.01655754,\n",
       "          0.0143347 , 0.01141898, 0.01369936, 0.01345371, 0.01623008,\n",
       "          0.01391732, 0.01471422, 0.01401911, 0.01708408, 0.01494988,\n",
       "          0.01374402, 0.02397634, 0.02485643, 0.02361094, 0.02823127,\n",
       "          0.02475332, 0.02269711, 0.02055177, 0.01851585, 0.01641657,\n",
       "          0.01723141, 0.01629075, 0.01736622, 0.01468788, 0.01342128,\n",
       "          0.01337762, 0.01255234, 0.01291729, 0.0123789 , 0.01476568,\n",
       "          0.01459014, 0.01902319, 0.01837427, 0.02054992, 0.01868908,\n",
       "          0.02422801, 0.0224257 , 0.02163549, 0.02198054, 0.02106187,\n",
       "          0.01974076, 0.02111818, 0.01948596, 0.01907778, 0.0177857 ,\n",
       "          0.01788711, 0.01745007, 0.02016772, 0.01982853, 0.02124476,\n",
       "          0.02101231, 0.01965683, 0.02046513, 0.02744673, 0.02590776,\n",
       "          0.02649212, 0.02473286, 0.01857916, 0.01583162, 0.01522531,\n",
       "          0.01537336, 0.01535784, 0.01310969, 0.01286761, 0.01764195,\n",
       "          0.01680711, 0.01780178, 0.01889216, 0.01804832, 0.01476293,\n",
       "          0.01375306, 0.01268023, 0.01294744, 0.01124243, 0.00960166,\n",
       "          0.0097729 , 0.00994649, 0.01010867, 0.00856993, 0.01187764,\n",
       "          0.01250284, 0.01108661, 0.01171694, 0.01192576, 0.01294755,\n",
       "          0.0152526 , 0.03556788, 0.03036546, 0.02363605, 0.02147342,\n",
       "          0.01683152, 0.01577739, 0.0185838 , 0.02200192, 0.02251438,\n",
       "          0.02327507, 0.0178875 , 0.01688809, 0.02043361, 0.02001673,\n",
       "          0.02428114, 0.02409838, 0.0229141 , 0.01793039, 0.01881173,\n",
       "          0.01613367, 0.01473028, 0.01430627, 0.01325326, 0.01318645,\n",
       "          0.01882509, 0.01645583, 0.01729832, 0.01613745, 0.01714308,\n",
       "          0.02037825, 0.01864747, 0.0163019 , 0.01569325, 0.01794268,\n",
       "          0.01504351, 0.01565963, 0.01550146, 0.01430138, 0.01280351,\n",
       "          0.01352357, 0.01335621, 0.01465005, 0.02118957, 0.02210062,\n",
       "          0.02351548, 0.02328718, 0.0274368 , 0.02461306, 0.03034963,\n",
       "          0.03237773, 0.03022182, 0.03459165, 0.03171327, 0.03887921,\n",
       "          0.03802903, 0.03505014, 0.0351242 , 0.03182271, 0.02717306,\n",
       "          0.02882723, 0.02662867, 0.02505959, 0.02434356, 0.01967867,\n",
       "          0.01782614, 0.02106131, 0.02090716, 0.02764865, 0.02838703,\n",
       "          0.02678651, 0.02610376, 0.0434267 , 0.06601042, 0.06864006,\n",
       "          0.07441597, 0.05643466, 0.06512158, 0.05701409, 0.05034646,\n",
       "          0.04953235, 0.05204827, 0.04251171, 0.0456508 , 0.04151664,\n",
       "          0.03749192, 0.03398431, 0.02829427, 0.02778119, 0.02640392,\n",
       "          0.02682118, 0.02423389, 0.021659  , 0.02139788, 0.02409152,\n",
       "          0.02280938, 0.02235418, 0.02169164, 0.01856841, 0.0178926 ,\n",
       "          0.01812929, 0.01681763, 0.01559255, 0.01498682, 0.01484851,\n",
       "          0.02109609, 0.02080542, 0.02377673, 0.01995274, 0.02105519,\n",
       "          0.01970771, 0.01915797, 0.01869871, 0.01688306, 0.01629988,\n",
       "          0.01691845, 0.01434697, 0.01463179, 0.01450262, 0.01471754,\n",
       "          0.01555193, 0.02040624, 0.01846364, 0.02268873, 0.02090637,\n",
       "          0.02122478, 0.01965347, 0.01662288, 0.01546318, 0.01481108,\n",
       "          0.01407193, 0.01388306, 0.01424832, 0.01326785, 0.01258265,\n",
       "          0.01521481, 0.01437601, 0.01310101, 0.01277295, 0.01416992,\n",
       "          0.01489303, 0.01565175, 0.01496284, 0.01400868, 0.014037  ,\n",
       "          0.01395695, 0.01343229, 0.01228848, 0.01241726, 0.01129101,\n",
       "          0.01187357, 0.01327395, 0.01276296, 0.0151518 , 0.02307354,\n",
       "          0.01900249, 0.0196837 , 0.01793233, 0.01764548, 0.01405576,\n",
       "          0.01578559, 0.01278645, 0.01199919, 0.01239038, 0.01180164,\n",
       "          0.0120433 , 0.01295249, 0.01357294, 0.01561503, 0.01242558,\n",
       "          0.01379959, 0.01331861, 0.01368191, 0.0126171 , 0.01412396,\n",
       "          0.01761061, 0.01581037, 0.01372528, 0.01355461, 0.01339613,\n",
       "          0.0124518 , 0.01299562, 0.01189366, 0.01203573, 0.01293481,\n",
       "          0.01358259, 0.0130969 , 0.01279211, 0.01191327, 0.0127425 ,\n",
       "          0.01406926, 0.01391173, 0.01553327, 0.01339238, 0.01473696,\n",
       "          0.01443665, 0.01410812, 0.01456364, 0.01450016, 0.01391581,\n",
       "          0.0151614 , 0.01339203, 0.01363694, 0.02582757, 0.02239514,\n",
       "          0.02467814, 0.02436054, 0.03015635, 0.02287207, 0.01940358,\n",
       "          0.01878537, 0.02071   , 0.01802415, 0.01398007, 0.01299561,\n",
       "          0.01347711, 0.01415886, 0.01516554, 0.01570685, 0.01534626,\n",
       "          0.01975593, 0.01804489, 0.02118389, 0.02069375, 0.01877979,\n",
       "          0.01753429, 0.01789274, 0.02231126, 0.02246981, 0.02660014,\n",
       "          0.0294279 , 0.02810594, 0.03149612, 0.02782172, 0.02423867,\n",
       "          0.02096198, 0.01864305, 0.01489783]),\n",
       "   'test': array([0.01665431, 0.01485784, 0.013029  , 0.01271307, 0.01079138,\n",
       "          0.01316204, 0.01395276, 0.01234726, 0.01068521, 0.01439104,\n",
       "          0.01529286, 0.01556628, 0.01923248, 0.01793432, 0.01426049,\n",
       "          0.01234268, 0.01171338, 0.00995951, 0.01045326, 0.01144468,\n",
       "          0.01179476, 0.01366183, 0.01195381, 0.01239551, 0.0120666 ,\n",
       "          0.01345448, 0.01361014, 0.01361014, 0.01201063, 0.01201063])}}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# time-series CV over ridge, lasso, elastic net (step-by-step)\n",
    "param_grids = {\n",
    "    'ridge': {\n",
    "        'model__alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 50.0, 100.0]\n",
    "    },\n",
    "    'lasso': {\n",
    "        'model__alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 50.0],\n",
    "        'model__max_iter': [5000]\n",
    "    },\n",
    "    'elasticnet': {\n",
    "        'model__alpha': [0.001, 0.01, 0.1, 1.0, 10.0],\n",
    "        'model__l1_ratio': [0.1, 0.5, 0.9],\n",
    "        'model__max_iter': [5000]\n",
    "    }\n",
    "}\n",
    "\n",
    "base_models = {\n",
    "    'ridge': Pipeline([('scaler', StandardScaler()), ('model', Ridge())]),\n",
    "    'lasso': Pipeline([('scaler', StandardScaler()), ('model', Lasso())]),\n",
    "    'elasticnet': Pipeline([('scaler', StandardScaler()), ('model', ElasticNet())])\n",
    "}\n",
    "\n",
    "# run CV for every combo\n",
    "cv_records = []\n",
    "for family, base_model in base_models.items():\n",
    "    grid = ParameterGrid(param_grids[family])\n",
    "    for params in grid:\n",
    "        candidate = clone(base_model).set_params(**params)\n",
    "        mean_rmse, std_rmse = ts_cv_score(candidate, X_train_raw, y_train, n_splits=5)\n",
    "        cv_records.append({\n",
    "            'family': family,\n",
    "            'params': params,\n",
    "            'cv_rmse_mean': mean_rmse,\n",
    "            'cv_rmse_std': std_rmse\n",
    "        })\n",
    "\n",
    "# pick the best CV combo for each family\n",
    "best_cv_per_family = {}\n",
    "for family in base_models.keys():\n",
    "    family_rows = [row for row in cv_records if row['family'] == family]\n",
    "    best_row = sorted(family_rows, key=lambda x: x['cv_rmse_mean'])[0]\n",
    "    best_cv_per_family[family] = best_row\n",
    "\n",
    "# fit tuned models using the best CV params\n",
    "family_tuned = {}\n",
    "for family, cv_info in best_cv_per_family.items():\n",
    "    tuned_model = clone(base_models[family]).set_params(**cv_info['params'])\n",
    "    tuned_metrics, tuned_fitted, tuned_preds = eval_model(\n",
    "        tuned_model,\n",
    "        f\"{family}_tuned\",\n",
    "        X_train_raw,\n",
    "        y_train,\n",
    "        X_val_raw,\n",
    "        y_val,\n",
    "        X_test_raw,\n",
    "        y_test\n",
    "    )\n",
    "    family_tuned[family] = {\n",
    "        'cv': cv_info,\n",
    "        'metrics': tuned_metrics,\n",
    "        'model': tuned_fitted,\n",
    "        'preds': tuned_preds\n",
    "    }\n",
    "\n",
    "# decide the best overall model by validation RMSE (baseline vs tuned)\n",
    "tuned_metrics_list = [v['metrics'] for v in family_tuned.values()]\n",
    "all_candidates = baseline_results + tuned_metrics_list\n",
    "best_overall = sorted(all_candidates, key=lambda x: x['val']['rmse'])[0]\n",
    "best_name = best_overall['name']\n",
    "\n",
    "# grab the fitted model and predictions\n",
    "model_pool = baseline_models + [(v['metrics'], v['model'], v['preds']) for v in family_tuned.values()]\n",
    "best_tuple = [entry for entry in model_pool if entry[0]['name'] == best_name][0]\n",
    "best_model = best_tuple[1]\n",
    "best_preds = best_tuple[2]\n",
    "\n",
    "best_cv_overall = best_cv_per_family[best_name.split('_')[0]] if best_name.endswith('tuned') else None\n",
    "\n",
    "family_tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03640a87",
   "metadata": {},
   "source": [
    "## Compare baseline vs tuned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf1bb98",
   "metadata": {},
   "source": [
    "> Model selection uses validation RMSE only (time-aware splits). Test set is held out strictly for final reporting, not for choosing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf89a3f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lasso_base</th>\n",
       "      <th>lasso_tuned</th>\n",
       "      <th>lasso_diff</th>\n",
       "      <th>ridge_base</th>\n",
       "      <th>ridge_tuned</th>\n",
       "      <th>ridge_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rmse</th>\n",
       "      <td>0.012286</td>\n",
       "      <td>0.009395</td>\n",
       "      <td>-0.002891</td>\n",
       "      <td>0.009371</td>\n",
       "      <td>0.009439</td>\n",
       "      <td>0.000068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mae</th>\n",
       "      <td>0.007610</td>\n",
       "      <td>0.005664</td>\n",
       "      <td>-0.001946</td>\n",
       "      <td>0.005724</td>\n",
       "      <td>0.005783</td>\n",
       "      <td>0.000059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r2</th>\n",
       "      <td>-0.001554</td>\n",
       "      <td>0.414331</td>\n",
       "      <td>0.415885</td>\n",
       "      <td>0.417381</td>\n",
       "      <td>0.408899</td>\n",
       "      <td>-0.008482</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      lasso_base  lasso_tuned  lasso_diff  ridge_base  ridge_tuned  ridge_diff\n",
       "rmse    0.012286     0.009395   -0.002891    0.009371     0.009439    0.000068\n",
       "mae     0.007610     0.005664   -0.001946    0.005724     0.005783    0.000059\n",
       "r2     -0.001554     0.414331    0.415885    0.417381     0.408899   -0.008482"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare baseline vs tuned (easy to read)\n",
    "families = ['lasso', 'ridge']\n",
    "metrics = ['rmse', 'mae', 'r2']\n",
    "\n",
    "# maps for quick lookup\n",
    "baseline_map = {m['name']: m for m in baseline_results}\n",
    "tuned_map = {v['metrics']['name']: v['metrics'] for v in family_tuned.values()}\n",
    "\n",
    "# build a simple table for a given split\n",
    "def build_split_table(split_name):\n",
    "    table = {}\n",
    "    for family in families:\n",
    "        base_key = f\"{family}_base\"\n",
    "        tuned_key = f\"{family}_tuned\"\n",
    "        base_metrics = [baseline_map[base_key][split_name][m] for m in metrics]\n",
    "        tuned_metrics_vals = [tuned_map[tuned_key][split_name][m] for m in metrics]\n",
    "        diff_metrics = [t - b for t, b in zip(tuned_metrics_vals, base_metrics)]\n",
    "        table[f\"{family}_base\"] = base_metrics\n",
    "        table[f\"{family}_tuned\"] = tuned_metrics_vals\n",
    "        table[f\"{family}_diff\"] = diff_metrics\n",
    "    return pd.DataFrame(table, index=metrics)\n",
    "\n",
    "val_comparison_df = build_split_table('val')\n",
    "test_comparison_df = build_split_table('test') if len(y_test) else None\n",
    "\n",
    "val_comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b64e319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lasso_base</th>\n",
       "      <th>lasso_tuned</th>\n",
       "      <th>lasso_diff</th>\n",
       "      <th>ridge_base</th>\n",
       "      <th>ridge_tuned</th>\n",
       "      <th>ridge_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rmse</th>\n",
       "      <td>0.009620</td>\n",
       "      <td>0.004834</td>\n",
       "      <td>-0.004785</td>\n",
       "      <td>0.004635</td>\n",
       "      <td>0.004490</td>\n",
       "      <td>-0.000145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mae</th>\n",
       "      <td>0.008887</td>\n",
       "      <td>0.003967</td>\n",
       "      <td>-0.004921</td>\n",
       "      <td>0.003977</td>\n",
       "      <td>0.003732</td>\n",
       "      <td>-0.000245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r2</th>\n",
       "      <td>-5.829308</td>\n",
       "      <td>-0.724685</td>\n",
       "      <td>5.104624</td>\n",
       "      <td>-0.585514</td>\n",
       "      <td>-0.487537</td>\n",
       "      <td>0.097977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      lasso_base  lasso_tuned  lasso_diff  ridge_base  ridge_tuned  ridge_diff\n",
       "rmse    0.009620     0.004834   -0.004785    0.004635     0.004490   -0.000145\n",
       "mae     0.008887     0.003967   -0.004921    0.003977     0.003732   -0.000245\n",
       "r2     -5.829308    -0.724685    5.104624   -0.585514    -0.487537    0.097977"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "558e61a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'ridge_base',\n",
       " 'train': {'rmse': 0.009478733598480976,\n",
       "  'mae': 0.006036318876028565,\n",
       "  'r2': 0.6072369626380871},\n",
       " 'val': {'rmse': 0.00937056442784859,\n",
       "  'mae': 0.005723706201454683,\n",
       "  'r2': 0.41738086626526394},\n",
       " 'test': {'rmse': 0.004635058164696176,\n",
       "  'mae': 0.003977044265744952,\n",
       "  'r2': -0.5855142983167421}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_overall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb312ab",
   "metadata": {},
   "source": [
    "## Feature importance from best model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05b1dd49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>coefficient</th>\n",
       "      <th>abs_coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vix3m</td>\n",
       "      <td>-0.011209</td>\n",
       "      <td>0.011209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rv_vix_spread_20d</td>\n",
       "      <td>-0.009704</td>\n",
       "      <td>0.009704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vix</td>\n",
       "      <td>0.009671</td>\n",
       "      <td>0.009671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spy_vol_10d</td>\n",
       "      <td>0.001641</td>\n",
       "      <td>0.001641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spy_ret_10d</td>\n",
       "      <td>-0.001495</td>\n",
       "      <td>0.001495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spy_vol_60d</td>\n",
       "      <td>0.001015</td>\n",
       "      <td>0.001015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>corr_spy_hyg_60d</td>\n",
       "      <td>-0.000773</td>\n",
       "      <td>0.000773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>spy_vol_20d</td>\n",
       "      <td>-0.000767</td>\n",
       "      <td>0.000767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>corr_spy_tlt_20d</td>\n",
       "      <td>0.000716</td>\n",
       "      <td>0.000716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>corr_spy_tlt_60d</td>\n",
       "      <td>-0.000693</td>\n",
       "      <td>0.000693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             feature  coefficient  abs_coefficient\n",
       "0              vix3m    -0.011209         0.011209\n",
       "1  rv_vix_spread_20d    -0.009704         0.009704\n",
       "2                vix     0.009671         0.009671\n",
       "3        spy_vol_10d     0.001641         0.001641\n",
       "4        spy_ret_10d    -0.001495         0.001495\n",
       "5        spy_vol_60d     0.001015         0.001015\n",
       "6   corr_spy_hyg_60d    -0.000773         0.000773\n",
       "7        spy_vol_20d    -0.000767         0.000767\n",
       "8   corr_spy_tlt_20d     0.000716         0.000716\n",
       "9   corr_spy_tlt_60d    -0.000693         0.000693"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get feature importance sorted by absolute value\n",
    "feature_names = list(X_train_raw.columns)\n",
    "importance_df = None\n",
    "\n",
    "model_has_coefs = hasattr(best_model.named_steps['model'], 'coef_')\n",
    "if model_has_coefs:\n",
    "    coefs = best_model.named_steps['model'].coef_\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'coefficient': coefs,\n",
    "        'abs_coefficient': np.abs(coefs)\n",
    "    }).sort_values('abs_coefficient', ascending=False).reset_index(drop=True)\n",
    "else:\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'note': 'no coefficients available for this model'\n",
    "    })\n",
    "\n",
    "importance_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855be1fe",
   "metadata": {},
   "source": [
    "## Save results and artifacts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2f4911c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ridge_base', {'alpha': 1.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save metrics, hyperparameters, model, and predictions\n",
    "metrics_payload = {\n",
    "    'base': {m['name']: m for m in baseline_results},\n",
    "    'tuned': {v['metrics']['name']: v['metrics'] for v in family_tuned.values()},\n",
    "    'best': best_overall,\n",
    "    'best_model': best_name,\n",
    "    'best_cv': best_cv_overall,\n",
    "    'val_comparison': val_comparison_df.to_dict(),\n",
    "    'test_comparison': test_comparison_df.to_dict() if test_comparison_df is not None else None\n",
    "}\n",
    "\n",
    "def pick_best_params(model_name):\n",
    "    family = model_name.split('_')[0]\n",
    "    # tuned models use the CV-chosen params\n",
    "    if model_name.endswith('tuned') and family in best_cv_per_family:\n",
    "        raw_params = best_cv_per_family[family]['params']\n",
    "    else:\n",
    "        # baselines grab whatever was in the grid for that family\n",
    "        grid_keys = list(param_grids.get(family, {}).keys())\n",
    "        baseline_entry = [item for item in baseline_models if item[0]['name'] == model_name][0][1]\n",
    "        raw_params = {key: baseline_entry.get_params().get(key) for key in grid_keys}\n",
    "    if not raw_params:\n",
    "        return None\n",
    "    clean_params = {key.split('__')[-1]: value for key, value in raw_params.items()}\n",
    "    return clean_params\n",
    "\n",
    "best_params = pick_best_params(best_name)\n",
    "\n",
    "hyperparams_payload = {\n",
    "    'best_model_name': best_name,\n",
    "    'best_params': best_params\n",
    "}\n",
    "\n",
    "with open(MODEL_DIR / 'metrics.json', 'w') as f:\n",
    "    json.dump(metrics_payload, f, indent=2)\n",
    "with open(MODEL_DIR / 'hyperparams.json', 'w') as f:\n",
    "    json.dump(hyperparams_payload, f, indent=2)\n",
    "\n",
    "# save feature importance\n",
    "if importance_df is not None:\n",
    "    importance_df.to_parquet(MODEL_DIR / 'feature_importance.parquet', index=False)\n",
    "    importance_df.to_csv(MODEL_DIR / 'feature_importance.csv', index=False)\n",
    "\n",
    "joblib.dump(best_model, MODEL_DIR / 'model.joblib')\n",
    "\n",
    "pd.DataFrame({\n",
    "    'pred_train': best_preds['train'],\n",
    "    'y_train': y_train\n",
    "}).to_parquet(MODEL_DIR / 'pred_train.parquet', index=False)\n",
    "pd.DataFrame({\n",
    "    'pred_val': best_preds['val'],\n",
    "    'y_val': y_val\n",
    "}).to_parquet(MODEL_DIR / 'pred_val.parquet', index=False)\n",
    "if best_preds['test'] is not None and len(best_preds['test']):\n",
    "    pd.DataFrame({\n",
    "        'pred_test': best_preds['test'],\n",
    "        'y_test': y_test\n",
    "    }).to_parquet(MODEL_DIR / 'pred_test.parquet', index=False)\n",
    "\n",
    "best_name, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d785a64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
