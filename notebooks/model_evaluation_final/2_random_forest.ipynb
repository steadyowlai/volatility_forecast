{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "290a3b5a",
   "metadata": {},
   "source": [
    "# Random forest regressor (tuned with OOB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9ee23b",
   "metadata": {},
   "source": [
    "Load splits from artifacts/data, run a solid RF baseline with OOB, tune with TimeSeriesSplit + OOB-aware settings, compare on val/test, and save outputs under artifacts/random_forest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9c44360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit, ParameterGrid\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86a729ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('/Users/aayushrijal/Documents/GitHub/volatility_forecast/notebooks/model_evaluation_final/artifacts/data'),\n",
       " PosixPath('/Users/aayushrijal/Documents/GitHub/volatility_forecast/notebooks/model_evaluation_final/artifacts/random_forest'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# paths to data and artifacts\n",
    "_start = Path.cwd().resolve()\n",
    "_candidates = [_start] + list(_start.parents)\n",
    "_repo = None\n",
    "for p in _candidates:\n",
    "    if (p / 'notebooks/model_evaluation_final/artifacts/data/split_config.json').exists():\n",
    "        _repo = p\n",
    "        break\n",
    "    if (p / 'data/master_dataset.parquet').exists():\n",
    "        _repo = p\n",
    "        break\n",
    "REPO_ROOT = _repo if _repo else _start\n",
    "ARTIFACTS_DIR = REPO_ROOT / 'notebooks/model_evaluation_final/artifacts'\n",
    "DATA_DIR = ARTIFACTS_DIR / 'data'\n",
    "MODEL_DIR = ARTIFACTS_DIR / 'random_forest'\n",
    "for d in [ARTIFACTS_DIR, DATA_DIR, MODEL_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_DIR, MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4289866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3154, 788, 30, 20)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load artifacts\n",
    "with open(DATA_DIR / 'feature_columns.json') as f:\n",
    "    feature_cols = json.load(f)\n",
    "with open(DATA_DIR / 'split_config.json') as f:\n",
    "    CONFIG = json.load(f)\n",
    "target_col = CONFIG['target_col']\n",
    "date_col = CONFIG['date_col']\n",
    "\n",
    "X_train = pd.read_parquet(DATA_DIR / 'X_train.parquet')\n",
    "X_val = pd.read_parquet(DATA_DIR / 'X_val.parquet')\n",
    "X_test = pd.read_parquet(DATA_DIR / 'X_test.parquet')\n",
    "y_train = pd.read_parquet(DATA_DIR / 'y_train.parquet')[target_col].values\n",
    "y_val = pd.read_parquet(DATA_DIR / 'y_val.parquet')[target_col].values\n",
    "y_test = pd.read_parquet(DATA_DIR / 'y_test.parquet')[target_col].values if (DATA_DIR / 'y_test.parquet').exists() else np.array([])\n",
    "\n",
    "len(X_train), len(X_val), len(X_test), len(feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78bbd0e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3154, 788, 30)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep raw features (RF handles scaling internally)\n",
    "X_train_raw = X_train.copy()\n",
    "X_val_raw = X_val.copy()\n",
    "X_test_raw = X_test.copy()\n",
    "has_test = len(X_test_raw) > 0\n",
    "\n",
    "len(X_train_raw), len(X_val_raw), len(X_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50bb342f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def metrics(y_true, y_pred):\n",
    "    return {\n",
    "        'rmse': float(np.sqrt(mean_squared_error(y_true, y_pred))),\n",
    "        'mae': float(mean_absolute_error(y_true, y_pred)),\n",
    "        'r2': float(r2_score(y_true, y_pred))\n",
    "    }\n",
    "\n",
    "def eval_model(model, name, X_tr, y_tr, X_v, y_v, X_te=None, y_te=None):\n",
    "    model.fit(X_tr, y_tr)\n",
    "    pred_tr = model.predict(X_tr)\n",
    "    pred_v = model.predict(X_v)\n",
    "    out = {\n",
    "        'name': name,\n",
    "        'train': metrics(y_tr, pred_tr),\n",
    "        'val': metrics(y_v, pred_v)\n",
    "    }\n",
    "    preds = {'train': pred_tr, 'val': pred_v, 'test': None}\n",
    "    if X_te is not None and len(X_te):\n",
    "        pred_te = model.predict(X_te)\n",
    "        out['test'] = metrics(y_te, pred_te)\n",
    "        preds['test'] = pred_te\n",
    "    if getattr(model, 'oob_prediction_', None) is not None:\n",
    "        out['oob'] = metrics(y_tr, model.oob_prediction_)\n",
    "    return out, model, preds\n",
    "\n",
    "def ts_cv_metrics(model, X, y, cv):\n",
    "    fold_metrics = []\n",
    "    for tr_idx, va_idx in cv.split(X):\n",
    "        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "        y_tr, y_va = y[tr_idx], y[va_idx]\n",
    "        m = clone(model)\n",
    "        m.fit(X_tr, y_tr)\n",
    "        preds = m.predict(X_va)\n",
    "        fold_metrics.append(metrics(y_va, preds))\n",
    "    agg = {}\n",
    "    for key in fold_metrics[0].keys():\n",
    "        vals = [fm[key] for fm in fold_metrics]\n",
    "        agg[key] = {'mean': float(np.mean(vals)), 'std': float(np.std(vals))}\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898605d9",
   "metadata": {},
   "source": [
    "## Baseline (default RF + OOB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14fec951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'rf_baseline',\n",
       " 'train': {'rmse': 0.0028086877020911945,\n",
       "  'mae': 0.001708142058771989,\n",
       "  'r2': 0.9655144719596567},\n",
       " 'val': {'rmse': 0.009302375771876566,\n",
       "  'mae': 0.00595361555718338,\n",
       "  'r2': 0.4258293364909922},\n",
       " 'test': {'rmse': 0.005005817616055346,\n",
       "  'mae': 0.00440487140243293,\n",
       "  'r2': -0.8493104793880328},\n",
       " 'oob': {'rmse': 0.007657993075988952,\n",
       "  'mae': 0.004652571885568823,\n",
       "  'r2': 0.7436343032470809}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_model = RandomForestRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=None,\n",
    "    max_features='sqrt',\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    bootstrap=True,\n",
    "    oob_score=True,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "baseline_out, baseline_model, baseline_preds = eval_model(\n",
    "    baseline_model,\n",
    "    'rf_baseline',\n",
    "    X_train_raw, y_train,\n",
    "    X_val_raw, y_val,\n",
    "    X_test_raw if has_test else None,\n",
    "    y_test if has_test else None\n",
    ")\n",
    "baseline_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0204eeca",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning (TimeSeriesSplit + OOB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6bb23a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                              params  \\\n",
       " 0  {\"bootstrap\": true, \"max_depth\": 16, \"max_feat...   \n",
       " 1  {\"bootstrap\": true, \"max_depth\": 16, \"max_feat...   \n",
       " 2  {\"bootstrap\": true, \"max_depth\": 16, \"max_feat...   \n",
       " 3  {\"bootstrap\": true, \"max_depth\": 16, \"max_feat...   \n",
       " 4  {\"bootstrap\": true, \"max_depth\": 16, \"max_feat...   \n",
       " \n",
       "                                                  oob  val_rmse_mean  \\\n",
       " 0  {'rmse': 0.00806107222567294, 'mae': 0.0049802...       0.010953   \n",
       " 1  {'rmse': 0.008069921470353736, 'mae': 0.004981...       0.010926   \n",
       " 2  {'rmse': 0.008108940226198716, 'mae': 0.005008...       0.010976   \n",
       " 3  {'rmse': 0.00811394775541727, 'mae': 0.0050051...       0.010953   \n",
       " 4  {'rmse': 0.008156604678132135, 'mae': 0.005007...       0.010915   \n",
       " \n",
       "    val_rmse_std  cv_rmse_mean  cv_rmse_std  \n",
       " 0      0.003407      0.010953     0.003407  \n",
       " 1      0.003416      0.010926     0.003416  \n",
       " 2      0.003315      0.010976     0.003315  \n",
       " 3      0.003319      0.010953     0.003319  \n",
       " 4      0.003530      0.010915     0.003530  ,\n",
       " {'bootstrap': True,\n",
       "  'max_depth': 18,\n",
       "  'max_features': 'sqrt',\n",
       "  'max_samples': 0.55,\n",
       "  'min_samples_leaf': 2,\n",
       "  'min_samples_split': 5,\n",
       "  'n_estimators': 300},\n",
       " {'mean': 0.010825617749792663, 'std': 0.0035283439960411985})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "#we had searched more broadly before, now tightening the grid\n",
    "param_grid = {\n",
    "    'n_estimators': [280, 300],\n",
    "    'max_depth': [16, 18],\n",
    "    'max_features': ['sqrt', 0.65],\n",
    "    'min_samples_split': [4, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'bootstrap': [True],\n",
    "    'max_samples': [0.55, 0.65]\n",
    "}\n",
    "\n",
    "cv_records = []\n",
    "best_entry = None\n",
    "for params in ParameterGrid(param_grid):\n",
    "    candidate = RandomForestRegressor(random_state=42, n_jobs=-1, oob_score=True, **params)\n",
    "    cv_stats = ts_cv_metrics(candidate, X_train_raw, y_train, tscv)\n",
    "    fitted = clone(candidate)\n",
    "    fitted.fit(X_train_raw, y_train)\n",
    "    oob_stats = None\n",
    "    if getattr(fitted, 'oob_prediction_', None) is not None:\n",
    "        oob_stats = metrics(y_train, fitted.oob_prediction_)\n",
    "    entry = {\n",
    "        'params': params,\n",
    "        'cv': cv_stats,\n",
    "        'oob': oob_stats,\n",
    "        'val_rmse_mean': cv_stats['rmse']['mean'],\n",
    "        'val_rmse_std': cv_stats['rmse']['std']\n",
    "    }\n",
    "    cv_records.append(entry)\n",
    "    if (best_entry is None) or (entry['val_rmse_mean'] < best_entry['val_rmse_mean']):\n",
    "        best_entry = {**entry, 'model': fitted}\n",
    "\n",
    "cv_results_df = pd.DataFrame([{**{k: v for k, v in rec.items() if k != 'cv'}, 'cv_rmse_mean': rec['cv']['rmse']['mean'], 'cv_rmse_std': rec['cv']['rmse']['std']} for rec in cv_records])\n",
    "# serialize params for parquet friendliness\n",
    "cv_results_df_serializable = cv_results_df.copy()\n",
    "cv_results_df_serializable['params'] = cv_results_df_serializable['params'].apply(json.dumps)\n",
    "best_params = best_entry['params']\n",
    "best_cv = best_entry['cv']\n",
    "best_oob = best_entry['oob']\n",
    "best_model = best_entry['model']\n",
    "cv_results_df_serializable.head(), best_params, best_cv['rmse']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e9bf39",
   "metadata": {},
   "source": [
    "## Evaluate tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a9eefeb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model selected: rf_baseline (by val RMSE)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'rf_baseline',\n",
       " 'train': {'rmse': 0.0028086877020911945,\n",
       "  'mae': 0.001708142058771989,\n",
       "  'r2': 0.9655144719596567},\n",
       " 'val': {'rmse': 0.009302375771876566,\n",
       "  'mae': 0.00595361555718338,\n",
       "  'r2': 0.4258293364909922},\n",
       " 'test': {'rmse': 0.005005817616055346,\n",
       "  'mae': 0.00440487140243293,\n",
       "  'r2': -0.8493104793880328},\n",
       " 'oob': {'rmse': 0.007657993075988952,\n",
       "  'mae': 0.004652571885568823,\n",
       "  'r2': 0.7436343032470809}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_pred_train = best_model.predict(X_train_raw)\n",
    "best_pred_val = best_model.predict(X_val_raw)\n",
    "best_pred_test = best_model.predict(X_test_raw) if has_test else None\n",
    "\n",
    "best_out = {\n",
    "    'name': 'rf_tuned',\n",
    "    'train': metrics(y_train, best_pred_train),\n",
    "    'val': metrics(y_val, best_pred_val)\n",
    "}\n",
    "best_preds = {'train': best_pred_train, 'val': best_pred_val, 'test': best_pred_test}\n",
    "if has_test:\n",
    "    best_out['test'] = metrics(y_test, best_pred_test)\n",
    "if best_oob is not None:\n",
    "    best_out['oob'] = best_oob\n",
    "\n",
    "# select best overall model vs baseline based on validation RMSE\n",
    "baseline_val_rmse = baseline_out['val']['rmse']\n",
    "tuned_val_rmse = best_out['val']['rmse']\n",
    "best_overall_name = 'rf_tuned' if tuned_val_rmse <= baseline_val_rmse else 'rf_baseline'\n",
    "if best_overall_name == 'rf_tuned':\n",
    "    best_overall_model = best_model\n",
    "    best_overall_preds = best_preds\n",
    "    best_overall_metrics = best_out\n",
    "    best_overall_params = best_params\n",
    "    best_overall_cv = best_cv\n",
    "else:\n",
    "    best_overall_model = baseline_model\n",
    "    best_overall_preds = baseline_preds\n",
    "    best_overall_metrics = baseline_out\n",
    "    best_overall_params = baseline_model.get_params()\n",
    "    best_overall_cv = None\n",
    "\n",
    "print(f\"Best model selected: {best_overall_name} (by val RMSE)\")\n",
    "best_overall_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0beb741",
   "metadata": {},
   "source": [
    "## Compare baseline vs tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42500813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation/Test comparison:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_rmse</th>\n",
       "      <th>val_mae</th>\n",
       "      <th>val_r2</th>\n",
       "      <th>test_rmse</th>\n",
       "      <th>test_mae</th>\n",
       "      <th>test_r2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rf_base</th>\n",
       "      <td>0.009302</td>\n",
       "      <td>0.005954</td>\n",
       "      <td>0.425829</td>\n",
       "      <td>0.005006</td>\n",
       "      <td>0.004405</td>\n",
       "      <td>-0.849310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf_tuned</th>\n",
       "      <td>0.009341</td>\n",
       "      <td>0.005924</td>\n",
       "      <td>0.421096</td>\n",
       "      <td>0.005098</td>\n",
       "      <td>0.004513</td>\n",
       "      <td>-0.917927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          val_rmse   val_mae    val_r2  test_rmse  test_mae   test_r2\n",
       "model                                                                \n",
       "rf_base   0.009302  0.005954  0.425829   0.005006  0.004405 -0.849310\n",
       "rf_tuned  0.009341  0.005924  0.421096   0.005098  0.004513 -0.917927"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned minus baseline (negative rmse/mae is better):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "val_rmse     0.000038\n",
       "val_mae     -0.000030\n",
       "val_r2      -0.004733\n",
       "test_rmse    0.000092\n",
       "test_mae     0.000108\n",
       "test_r2     -0.068617\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rows = []\n",
    "for label, res in [('rf_base', baseline_out), ('rf_tuned', best_out)]:\n",
    "    row = {\n",
    "        'model': label,\n",
    "        'val_rmse': res['val']['rmse'],\n",
    "        'val_mae': res['val']['mae'],\n",
    "        'val_r2': res['val']['r2']\n",
    "    }\n",
    "    if has_test and 'test' in res:\n",
    "        row['test_rmse'] = res['test']['rmse']\n",
    "        row['test_mae'] = res['test']['mae']\n",
    "        row['test_r2'] = res['test']['r2']\n",
    "    rows.append(row)\n",
    "comparison_df = pd.DataFrame(rows).set_index('model')\n",
    "print('Validation/Test comparison:')\n",
    "display(comparison_df)\n",
    "\n",
    "if 'rf_base' in comparison_df.index and 'rf_tuned' in comparison_df.index:\n",
    "    diff_df = comparison_df.loc['rf_tuned'] - comparison_df.loc['rf_base']\n",
    "    print('Tuned minus baseline (negative rmse/mae is better):')\n",
    "    display(diff_df)\n",
    "else:\n",
    "    print('Cannot compute diff; missing one of rf_base or rf_tuned.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe11672",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b121cd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vix</td>\n",
       "      <td>0.120040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rv_vix_spread_20d</td>\n",
       "      <td>0.116001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spy_vol_5d</td>\n",
       "      <td>0.099204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>drawdown_60d</td>\n",
       "      <td>0.075776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vix_term</td>\n",
       "      <td>0.068570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spy_vol_10d</td>\n",
       "      <td>0.068437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>vix3m</td>\n",
       "      <td>0.058747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>spy_ret_10d</td>\n",
       "      <td>0.058530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spy_vol_20d</td>\n",
       "      <td>0.045328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spy_ret_20d</td>\n",
       "      <td>0.042826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>spy_ret_60d</td>\n",
       "      <td>0.034883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>corr_spy_hyg_60d</td>\n",
       "      <td>0.032642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>spy_vol_60d</td>\n",
       "      <td>0.027197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rsi_spy_14</td>\n",
       "      <td>0.026655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>spy_ret_5d</td>\n",
       "      <td>0.026257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              feature  importance\n",
       "0                 vix    0.120040\n",
       "1   rv_vix_spread_20d    0.116001\n",
       "2          spy_vol_5d    0.099204\n",
       "3        drawdown_60d    0.075776\n",
       "4            vix_term    0.068570\n",
       "5         spy_vol_10d    0.068437\n",
       "6               vix3m    0.058747\n",
       "7         spy_ret_10d    0.058530\n",
       "8         spy_vol_20d    0.045328\n",
       "9         spy_ret_20d    0.042826\n",
       "10        spy_ret_60d    0.034883\n",
       "11   corr_spy_hyg_60d    0.032642\n",
       "12        spy_vol_60d    0.027197\n",
       "13         rsi_spy_14    0.026655\n",
       "14         spy_ret_5d    0.026257"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance_df = None\n",
    "if hasattr(best_overall_model, 'feature_importances_'):\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': best_overall_model.feature_importances_\n",
    "    }).sort_values(by='importance', ascending=False).reset_index(drop=True)\n",
    "importance_df.head(15) if importance_df is not None else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e9d921",
   "metadata": {},
   "source": [
    "## Save artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4adbe5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model: rf_baseline\n",
      "Hyperparameters:\n",
      "{\n",
      "  \"bootstrap\": true,\n",
      "  \"ccp_alpha\": 0.0,\n",
      "  \"criterion\": \"squared_error\",\n",
      "  \"max_depth\": null,\n",
      "  \"max_features\": \"sqrt\",\n",
      "  \"max_leaf_nodes\": null,\n",
      "  \"max_samples\": null,\n",
      "  \"min_impurity_decrease\": 0.0,\n",
      "  \"min_samples_leaf\": 1,\n",
      "  \"min_samples_split\": 2,\n",
      "  \"min_weight_fraction_leaf\": 0.0,\n",
      "  \"monotonic_cst\": null,\n",
      "  \"n_estimators\": 200,\n",
      "  \"n_jobs\": -1,\n",
      "  \"oob_score\": true,\n",
      "  \"random_state\": 42,\n",
      "  \"verbose\": 0,\n",
      "  \"warm_start\": false\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'criterion': 'squared_error',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'monotonic_cst': None,\n",
       " 'n_estimators': 200,\n",
       " 'n_jobs': -1,\n",
       " 'oob_score': True,\n",
       " 'random_state': 42,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Saving best model: {best_overall_name}\")\n",
    "print(\"Hyperparameters:\")\n",
    "print(json.dumps(best_overall_params, indent=2))\n",
    "\n",
    "metrics_payload = {\n",
    "    'base': baseline_out,\n",
    "    'tuned': best_out,\n",
    "    'best': best_overall_metrics,\n",
    "    'best_model': best_overall_name,\n",
    "    'best_cv': best_overall_cv\n",
    "}\n",
    "hyperparams_payload = {\n",
    "    'best_model_name': best_overall_name,\n",
    "    'best_params': best_overall_params\n",
    "}\n",
    "\n",
    "with open(MODEL_DIR / 'metrics.json', 'w') as f:\n",
    "    json.dump(metrics_payload, f, indent=2)\n",
    "with open(MODEL_DIR / 'hyperparams.json', 'w') as f:\n",
    "    json.dump(hyperparams_payload, f, indent=2)\n",
    "\n",
    "if importance_df is not None:\n",
    "    importance_df.to_parquet(MODEL_DIR / 'feature_importance.parquet', index=False)\n",
    "    importance_df.to_csv(MODEL_DIR / 'feature_importance.csv', index=False)\n",
    "\n",
    "joblib.dump(best_overall_model, MODEL_DIR / 'model.joblib')\n",
    "\n",
    "pred_frames = [\n",
    "    pd.DataFrame({'split': 'train', 'y_true': y_train, 'y_pred': best_overall_preds['train']}),\n",
    "    pd.DataFrame({'split': 'val', 'y_true': y_val, 'y_pred': best_overall_preds['val']})\n",
    "]\n",
    "if has_test and best_overall_preds['test'] is not None:\n",
    "    pred_frames.append(pd.DataFrame({'split': 'test', 'y_true': y_test, 'y_pred': best_overall_preds['test']}))\n",
    "pd.concat(pred_frames, ignore_index=True).to_parquet(MODEL_DIR / 'preds.parquet', index=False)\n",
    "\n",
    "# keep saving CV results from tuning run\n",
    "cv_results_df_serializable.to_parquet(MODEL_DIR / 'cv_results.parquet', index=False)\n",
    "\n",
    "best_overall_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbb38b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
