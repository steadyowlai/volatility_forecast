{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "741792c6",
   "metadata": {},
   "source": [
    "# Ensemble of top individual models\n",
    "Load the top-two individual models from `artifacts/best_individual_model`, stack them with a light meta-learner, and save the winner (ensemble vs best single) under `artifacts/best_overall_model`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899ae244",
   "metadata": {},
   "source": [
    "### Plan (keep the loose style from 3_xgboost)\n",
    "- Pull top-two picks from `best_individual_model` (no other folders) and load train/val/test from `artifacts/data` saved by 0_data_prep.ipynb.\n",
    "- Load their stored predictions, align val/test, and train a Ridge stacker.\n",
    "- Also try a simple blend (weighted average) on val to see if it beats stacking.\n",
    "- Compare all vs best individual by val RMSE (tie test RMSE if present), then drop results + hyperparams into `best_overall_model`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbefd864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57767249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('/Users/aayushrijal/Documents/GitHub/volatility_forecast/notebooks/model_evaluation_final/artifacts/best_individual_model'),\n",
       " PosixPath('/Users/aayushrijal/Documents/GitHub/volatility_forecast/notebooks/model_evaluation_final/artifacts/best_overall_model'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# paths to artifacts (same path dance as other notebooks)\n",
    "_start = Path.cwd().resolve()\n",
    "_candidates = [_start] + list(_start.parents)\n",
    "_repo = None\n",
    "for p in _candidates:\n",
    "    if (p / 'notebooks/model_evaluation_final/artifacts/best_individual_model/metrics.json').exists():\n",
    "        _repo = p\n",
    "        break\n",
    "    if (p / 'data/master_dataset.parquet').exists():\n",
    "        _repo = p\n",
    "        break\n",
    "REPO_ROOT = _repo if _repo else _start\n",
    "ARTIFACTS_DIR = REPO_ROOT / 'notebooks/model_evaluation_final/artifacts'\n",
    "DATA_DIR = ARTIFACTS_DIR / 'data'\n",
    "BEST_INDIVIDUAL_DIR = ARTIFACTS_DIR / 'best_individual_model'\n",
    "BEST_OVERALL_DIR = ARTIFACTS_DIR / 'best_overall_model'\n",
    "BEST_OVERALL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BEST_INDIVIDUAL_DIR, BEST_OVERALL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa349956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from artifacts/data:\n",
      "  X_val shape: (788, 20)\n",
      "  y_val len  : 788\n",
      "  has_test   : True\n"
     ]
    }
   ],
   "source": [
    "# load data artifacts from 0_data_prep output (for sanity)\n",
    "with open(DATA_DIR / 'feature_columns.json') as f:\n",
    "    feature_cols = json.load(f)\n",
    "with open(DATA_DIR / 'split_config.json') as f:\n",
    "    split_cfg = json.load(f)\n",
    "\n",
    "y_train = pd.read_parquet(DATA_DIR / 'y_train.parquet')[split_cfg['target_col']].values\n",
    "y_val = pd.read_parquet(DATA_DIR / 'y_val.parquet')[split_cfg['target_col']].values\n",
    "X_val_shape = pd.read_parquet(DATA_DIR / 'X_val.parquet').shape\n",
    "has_test = (DATA_DIR / 'y_test.parquet').exists()\n",
    "y_test = pd.read_parquet(DATA_DIR / 'y_test.parquet')[split_cfg['target_col']].values if has_test else np.array([])\n",
    "\n",
    "print('Loaded data from artifacts/data:')\n",
    "print('  X_val shape:', X_val_shape)\n",
    "print('  y_val len  :', len(y_val))\n",
    "print('  has_test   :', has_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84571120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top picks:\n",
      "  first : {'folder': 'xgboost', 'name': 'xgb_tuned', 'train': {'rmse': 0.008045408197754638, 'mae': 0.005604441560017746, 'r2': 0.7170392862840006}, 'val': {'rmse': 0.00907279671175083, 'mae': 0.00558345122421248, 'r2': 0.453820241414309}, 'test': {'rmse': 0.004869681452443808, 'mae': 0.0041195188750506005, 'r2': -0.7500920503923685}}\n",
      "  second: {'folder': 'lightgbm', 'name': 'lgbm_tuned', 'train': {'rmse': 0.009036037548994211, 'mae': 0.0056933769244386125, 'r2': 0.6430675534761805}, 'val': {'rmse': 0.009286497847619708, 'mae': 0.005615033211137025, 'r2': 0.4277877303640396}, 'test': {'rmse': 0.004717498467452408, 'mae': 0.003922820627835008, 'r2': -0.6424165838684288}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'best_model_name': 'xgb_tuned',\n",
       "  'best_params': {'colsample_bytree': 0.7,\n",
       "   'gamma': 0,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 4,\n",
       "   'n_estimators': 300,\n",
       "   'reg_lambda': 1.5,\n",
       "   'subsample': 0.8}},\n",
       " {'best_model_name': 'lgbm_tuned',\n",
       "  'best_params': {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_samples': 40,\n",
       "   'n_estimators': 300,\n",
       "   'num_leaves': 31,\n",
       "   'reg_alpha': 0.1,\n",
       "   'reg_lambda': 2.0,\n",
       "   'subsample': 0.7}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load top-two metadata and hyperparameters\n",
    "with open(BEST_INDIVIDUAL_DIR / 'metrics.json') as f:\n",
    "    best_metrics = json.load(f)\n",
    "with open(BEST_INDIVIDUAL_DIR / 'best_hyperparams.json') as f:\n",
    "    best_hyper = json.load(f)\n",
    "\n",
    "top_first = best_metrics.get('first')\n",
    "top_second = best_metrics.get('second')\n",
    "assert top_first is not None, 'No first-ranked model found in best_individual_model/metrics.json'\n",
    "\n",
    "print('Top picks:')\n",
    "print('  first :', top_first)\n",
    "print('  second:', top_second)\n",
    "best_hyper.get('first'), best_hyper.get('second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "611f0842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper: metrics + IO (keep it simple)\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        'rmse': float(np.sqrt(mean_squared_error(y_true, y_pred))),\n",
    "        'mae': float(mean_absolute_error(y_true, y_pred)),\n",
    "        'r2': float(r2_score(y_true, y_pred))\n",
    "    }\n",
    "\n",
    "def load_preds(folder_name: str):\n",
    "    folder_path = ARTIFACTS_DIR / folder_name\n",
    "    preds_path = folder_path / 'preds.parquet'\n",
    "    if not preds_path.exists():\n",
    "        raise FileNotFoundError(f\"Predictions file not found for {folder_name}: {preds_path}\")\n",
    "    df = pd.read_parquet(preds_path)\n",
    "    # Expect columns: split, y_true, y_pred\n",
    "    return df\n",
    "\n",
    "def align_split(pred_a: pd.DataFrame, pred_b: pd.DataFrame, split: str):\n",
    "    a = pred_a[pred_a['split'] == split].reset_index(drop=True)\n",
    "    b = pred_b[pred_b['split'] == split].reset_index(drop=True)\n",
    "    if len(a) == 0 or len(b) == 0:\n",
    "        return None\n",
    "    if len(a) != len(b):\n",
    "        raise ValueError(f'Mismatch in {split} rows between models: {len(a)} vs {len(b)}')\n",
    "    if not np.allclose(a['y_true'], b['y_true']):\n",
    "        raise ValueError(f'y_true mismatch for split={split}')\n",
    "    out = pd.DataFrame({\n",
    "        'y_true': a['y_true'].values,\n",
    "        f\"pred_{top_first['folder']}\": a['y_pred'].values,\n",
    "        f\"pred_{top_second['folder'] if top_second else 'second'}\": b['y_pred'].values\n",
    "    })\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73b71838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val rows: 788\n",
      "test rows: 30\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_true</th>\n",
       "      <th>pred_xgboost</th>\n",
       "      <th>pred_lightgbm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.043646</td>\n",
       "      <td>0.037810</td>\n",
       "      <td>0.041518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.044733</td>\n",
       "      <td>0.037102</td>\n",
       "      <td>0.036367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.045174</td>\n",
       "      <td>0.040993</td>\n",
       "      <td>0.041840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.037856</td>\n",
       "      <td>0.035198</td>\n",
       "      <td>0.034689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.038448</td>\n",
       "      <td>0.034968</td>\n",
       "      <td>0.033638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     y_true  pred_xgboost  pred_lightgbm\n",
       "0  0.043646      0.037810       0.041518\n",
       "1  0.044733      0.037102       0.036367\n",
       "2  0.045174      0.040993       0.041840\n",
       "3  0.037856      0.035198       0.034689\n",
       "4  0.038448      0.034968       0.033638"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load predictions for the top two models\n",
    "preds_first = load_preds(top_first['folder'])\n",
    "preds_second = load_preds(top_second['folder']) if top_second else None\n",
    "\n",
    "val_table = align_split(preds_first, preds_second, 'val') if preds_second is not None else None\n",
    "test_table = align_split(preds_first, preds_second, 'test') if preds_second is not None else None\n",
    "print('val rows:', len(val_table) if val_table is not None else 0)\n",
    "print('test rows:', len(test_table) if test_table is not None else 0)\n",
    "val_table.head() if val_table is not None else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "318c37bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble (val) metrics: {'rmse': 0.009929750314505598, 'mae': 0.005936287309028706, 'r2': 0.34577083981396606}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rmse': 0.009929750314505598,\n",
       " 'mae': 0.005936287309028706,\n",
       " 'r2': 0.34577083981396606}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit meta-learner on validation predictions\n",
    "if val_table is None:\n",
    "    raise ValueError('Validation predictions missing for one of the top models; cannot build ensemble.')\n",
    "\n",
    "X_val = val_table[[c for c in val_table.columns if c.startswith('pred_')]].values\n",
    "y_val = val_table['y_true'].values\n",
    "meta_model = Ridge(alpha=0.1)\n",
    "meta_model.fit(X_val, y_val)\n",
    "\n",
    "ensemble_val_pred = meta_model.predict(X_val)\n",
    "ensemble_val_metrics = compute_metrics(y_val, ensemble_val_pred)\n",
    "print('Ensemble (val) metrics:', ensemble_val_metrics)\n",
    "ensemble_val_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d91c3308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blend (val) weight: 1.0\n",
      "Blend (val) metrics: {'rmse': 0.00907279671175083, 'mae': 0.00558345122421248, 'r2': 0.453820241414309}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rmse': 0.00907279671175083,\n",
       " 'mae': 0.00558345122421248,\n",
       " 'r2': 0.453820241414309}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple blend (weighted average) on validation\n",
    "pred_cols = [c for c in val_table.columns if c.startswith('pred_')]\n",
    "assert len(pred_cols) == 2, 'Expected exactly two prediction columns for blending'\n",
    "p1 = val_table[pred_cols[0]].values\n",
    "p2 = val_table[pred_cols[1]].values\n",
    "weights = np.linspace(0, 1, 21)  # 0.0 to 1.0 step 0.05\n",
    "\n",
    "best_blend = None\n",
    "for w in weights:\n",
    "    blended = w * p1 + (1 - w) * p2\n",
    "    m = compute_metrics(y_val, blended)\n",
    "    if (best_blend is None) or (m['rmse'] < best_blend['metrics']['rmse']):\n",
    "        best_blend = {'weight': float(w), 'metrics': m}\n",
    "\n",
    "blend_weight = best_blend['weight']\n",
    "blend_val_metrics = best_blend['metrics']\n",
    "print('Blend (val) weight:', blend_weight)\n",
    "print('Blend (val) metrics:', blend_val_metrics)\n",
    "blend_val_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69dae324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble (test) metrics: {'rmse': 0.006828471173684929, 'mae': 0.005975153346164495, 'r2': -2.4411749026122562}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rmse': 0.006828471173684929,\n",
       " 'mae': 0.005975153346164495,\n",
       " 'r2': -2.4411749026122562}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate ensemble on test split if available\n",
    "ensemble_test_metrics = None\n",
    "if test_table is not None:\n",
    "    X_test = test_table[[c for c in test_table.columns if c.startswith('pred_')]].values\n",
    "    y_test = test_table['y_true'].values\n",
    "    ensemble_test_pred = meta_model.predict(X_test)\n",
    "    ensemble_test_metrics = compute_metrics(y_test, ensemble_test_pred)\n",
    "\n",
    "print('Ensemble (test) metrics:', ensemble_test_metrics)\n",
    "ensemble_test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc9b0316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blend (test) metrics: {'rmse': 0.004869681452443808, 'mae': 0.0041195188750506005, 'r2': -0.7500920503923685}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rmse': 0.004869681452443808,\n",
       " 'mae': 0.0041195188750506005,\n",
       " 'r2': -0.7500920503923685}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# blend test metrics using same weight\n",
    "blend_test_metrics = None\n",
    "if test_table is not None:\n",
    "    p1_test = test_table[pred_cols[0]].values\n",
    "    p2_test = test_table[pred_cols[1]].values\n",
    "    blended_test = blend_weight * p1_test + (1 - blend_weight) * p2_test\n",
    "    blend_test_metrics = compute_metrics(test_table['y_true'].values, blended_test)\n",
    "\n",
    "print('Blend (test) metrics:', blend_test_metrics)\n",
    "blend_test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2df4f85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison (val RMSE):\n",
      "  best_individual: 0.00907279671175083\n",
      "  stacked_ensemble: 0.009929750314505598\n",
      "  blended_ensemble: 0.00907279671175083\n",
      "Winner: best_individual\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_individual_val_rmse': 0.00907279671175083,\n",
       " 'stacked_val_rmse': 0.009929750314505598,\n",
       " 'blended_val_rmse': 0.00907279671175083,\n",
       " 'winner': 'xgb_tuned'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare candidates: best individual vs stacking vs blending\n",
    "candidates = {\n",
    "    'best_individual': {\n",
    "        'name': top_first['name'],\n",
    "        'source': f\"{top_first['folder']}/{top_first['name']}\",\n",
    "        'train': top_first.get('train'),\n",
    "        'val': top_first['val'],\n",
    "        'test': top_first.get('test')\n",
    "    },\n",
    "    'stacked_ensemble': {\n",
    "        'name': 'stacked_ensemble',\n",
    "        'source': 'stacking_ridge',\n",
    "        'train': None,  # meta-learner trained on val only\n",
    "        'val': ensemble_val_metrics,\n",
    "        'test': ensemble_test_metrics\n",
    "    },\n",
    "    'blended_ensemble': {\n",
    "        'name': 'blended_ensemble',\n",
    "        'source': f\"blend_weight_{blend_weight}\",\n",
    "        'train': None,\n",
    "        'val': blend_val_metrics,\n",
    "        'test': blend_test_metrics,\n",
    "        'blend_weight': blend_weight\n",
    "    }\n",
    "}\n",
    "\n",
    "# pick best by val rmse, tie-break on test rmse when both available\n",
    "best_key = None\n",
    "for key, entry in candidates.items():\n",
    "    if best_key is None:\n",
    "        best_key = key\n",
    "        continue\n",
    "    cur = entry\n",
    "    best = candidates[best_key]\n",
    "    if cur['val']['rmse'] < best['val']['rmse']:\n",
    "        best_key = key\n",
    "    elif np.isclose(cur['val']['rmse'], best['val']['rmse']):\n",
    "        cur_test = cur.get('test')\n",
    "        best_test = best.get('test')\n",
    "        if cur_test is not None and best_test is not None and cur_test.get('rmse') is not None and best_test.get('rmse') is not None:\n",
    "            if cur_test['rmse'] < best_test['rmse']:\n",
    "                best_key = key\n",
    "\n",
    "best_entry = candidates[best_key]\n",
    "print('Comparison (val RMSE):')\n",
    "for k, v in candidates.items():\n",
    "    print(f\"  {k}: {v['val']['rmse']}\")\n",
    "print('Winner:', best_key)\n",
    "\n",
    "best_overall_name = best_entry['name']\n",
    "best_overall_source = best_entry['source']\n",
    "best_overall_metrics = best_entry['val']\n",
    "best_overall_train = best_entry.get('train')\n",
    "best_overall_test = best_entry.get('test')\n",
    "comparison = {\n",
    "    'best_individual_val_rmse': candidates['best_individual']['val']['rmse'],\n",
    "    'stacked_val_rmse': candidates['stacked_ensemble']['val']['rmse'],\n",
    "    'blended_val_rmse': candidates['blended_ensemble']['val']['rmse'],\n",
    "    'winner': best_overall_name\n",
    "}\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98418e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best-overall bundle to /Users/aayushrijal/Documents/GitHub/volatility_forecast/notebooks/model_evaluation_final/artifacts/best_overall_model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_overall': {'type': 'individual',\n",
       "  'name': 'xgb_tuned',\n",
       "  'source': 'xgboost/xgb_tuned',\n",
       "  'train': {'rmse': 0.008045408197754638,\n",
       "   'mae': 0.005604441560017746,\n",
       "   'r2': 0.7170392862840006},\n",
       "  'val': {'rmse': 0.00907279671175083,\n",
       "   'mae': 0.00558345122421248,\n",
       "   'r2': 0.453820241414309},\n",
       "  'test': {'rmse': 0.004869681452443808,\n",
       "   'mae': 0.0041195188750506005,\n",
       "   'r2': -0.7500920503923685}},\n",
       " 'timestamp': '2026-02-03T22:37:49.935064Z'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# persist results to best_overall_model (only the winner)\n",
    "timestamp = datetime.utcnow().isoformat() + 'Z'\n",
    "is_stacked = best_key == 'stacked_ensemble'\n",
    "is_blended = best_key == 'blended_ensemble'\n",
    "\n",
    "if is_stacked:\n",
    "    metrics_payload = {\n",
    "        'best_overall': {\n",
    "            'type': 'stacked_ensemble',\n",
    "            'name': best_overall_name,\n",
    "            'source': best_overall_source,\n",
    "            'train': None,  # meta-learner uses val only\n",
    "            'val': ensemble_val_metrics,\n",
    "            'test': ensemble_test_metrics,\n",
    "            'components': {\n",
    "                'first': {\n",
    "                    'name': top_first['name'],\n",
    "                    'source': f\"{top_first['folder']}/{top_first['name']}\",\n",
    "                    'train': top_first.get('train'),\n",
    "                    'val': top_first['val'],\n",
    "                    'test': top_first.get('test')\n",
    "                },\n",
    "                'second': {\n",
    "                    'name': top_second['name'],\n",
    "                    'source': f\"{top_second['folder']}/{top_second['name']}\",\n",
    "                    'train': top_second.get('train'),\n",
    "                    'val': top_second['val'],\n",
    "                    'test': top_second.get('test')\n",
    "                } if top_second is not None else None\n",
    "            }\n",
    "        },\n",
    "        'timestamp': timestamp\n",
    "    }\n",
    "    hyperparams_payload = {\n",
    "        'best_overall': {\n",
    "            'type': 'stacked_ensemble',\n",
    "            'meta': {\n",
    "                'type': 'Ridge',\n",
    "                'params': meta_model.get_params()\n",
    "            },\n",
    "            'components': {\n",
    "                'first': best_hyper.get('first'),\n",
    "                'second': best_hyper.get('second') if top_second is not None else None\n",
    "            }\n",
    "        },\n",
    "        'timestamp': timestamp\n",
    "    }\n",
    "elif is_blended:\n",
    "    metrics_payload = {\n",
    "        'best_overall': {\n",
    "            'type': 'blended_ensemble',\n",
    "            'name': best_overall_name,\n",
    "            'source': best_overall_source,\n",
    "            'blend_weight': blend_weight,\n",
    "            'train': None,\n",
    "            'val': blend_val_metrics,\n",
    "            'test': blend_test_metrics,\n",
    "            'components': {\n",
    "                'first': {\n",
    "                    'name': top_first['name'],\n",
    "                    'source': f\"{top_first['folder']}/{top_first['name']}\",\n",
    "                    'train': top_first.get('train'),\n",
    "                    'val': top_first['val'],\n",
    "                    'test': top_first.get('test')\n",
    "                },\n",
    "                'second': {\n",
    "                    'name': top_second['name'],\n",
    "                    'source': f\"{top_second['folder']}/{top_second['name']}\",\n",
    "                    'train': top_second.get('train'),\n",
    "                    'val': top_second['val'],\n",
    "                    'test': top_second.get('test')\n",
    "                } if top_second is not None else None\n",
    "            }\n",
    "        },\n",
    "        'timestamp': timestamp\n",
    "    }\n",
    "    hyperparams_payload = {\n",
    "        'best_overall': {\n",
    "            'type': 'blended_ensemble',\n",
    "            'blend': {\n",
    "                'weight': blend_weight\n",
    "            },\n",
    "            'components': {\n",
    "                'first': best_hyper.get('first'),\n",
    "                'second': best_hyper.get('second') if top_second is not None else None\n",
    "            }\n",
    "        },\n",
    "        'timestamp': timestamp\n",
    "    }\n",
    "else:\n",
    "    metrics_payload = {\n",
    "        'best_overall': {\n",
    "            'type': 'individual',\n",
    "            'name': top_first['name'],\n",
    "            'source': f\"{top_first['folder']}/{top_first['name']}\",\n",
    "            'train': top_first.get('train'),\n",
    "            'val': top_first['val'],\n",
    "            'test': top_first.get('test')\n",
    "        },\n",
    "        'timestamp': timestamp\n",
    "    }\n",
    "    hyperparams_payload = {\n",
    "        'best_overall': {\n",
    "            'type': 'individual',\n",
    "            'hyperparams': best_hyper.get('first')\n",
    "        },\n",
    "        'timestamp': timestamp\n",
    "    }\n",
    "\n",
    "with open(BEST_OVERALL_DIR / 'metrics.json', 'w') as f:\n",
    "    json.dump(metrics_payload, f, indent=2)\n",
    "with open(BEST_OVERALL_DIR / 'hyperparams.json', 'w') as f:\n",
    "    json.dump(hyperparams_payload, f, indent=2)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'candidate': ['best_individual', 'stacked_ensemble', 'blended_ensemble'],\n",
    "    'val_rmse': [\n",
    "        candidates['best_individual']['val']['rmse'],\n",
    "        candidates['stacked_ensemble']['val']['rmse'],\n",
    "        candidates['blended_ensemble']['val']['rmse']\n",
    "    ]\n",
    "})\n",
    "comparison_df.to_csv(BEST_OVERALL_DIR / 'comparison.csv', index=False)\n",
    "\n",
    "print(f\"Saved best-overall bundle to {BEST_OVERALL_DIR}\")\n",
    "metrics_payload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7108fdf",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- Ranking uses validation RMSE primary, test RMSE (if present) secondary.\n",
    "- Base model picks come only from `best_individual_model` (top two). Data sanity pulled from `artifacts/data` (0_data_prep output).\n",
    "- Two ensemble flavors: Ridge stacker and weighted blend (grid over 0..1). Winner is whichever beats the others on val RMSE (tie-break test).\n",
    "- Adjust Ridge alpha or blend grid if you want finer search."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
